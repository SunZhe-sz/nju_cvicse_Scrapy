{"timestamp": "2018_08_13_17_28_53", "desc": "重点在于CrawlSpider的学习！！！！！！！！！！！！！\n\n\n\n**通过前面的学习我们可以进行一些页面的简单自动话爬取，对于一些比较规则的网站，我们似乎可以用Spider类去应付，可是，对于一些较为复杂或者说链接的存放不规则的网站我们该怎么去爬取呢，接下来的爬虫就是要解决这个问题，而且还可以高度的自动化爬取链接和链接内容**\n\n\nCrawlSpider类，是建立爬虫的另外一个类。\n\n*（顺便说一下，我们可以继承四种类来建立我们的scrapy爬虫，他们是：Spider类，CrawlSpider类， CSVFeedSpider类和XMLFeedSpider类，今天我们讲的就是CrawlSpider类建立的爬虫）*\n\n\nCrawlSpider类通过一些规则（rules），使对于链接（网页）的爬取更具有通用性，换句话说，CrawlSpider爬虫为通用性的爬虫，而Spider爬虫更像是为一些特殊网站制定的爬虫。\n\n那我们开始正式的讲解一下CrawlSpider爬虫。。。。\n\n\n\n首先我们建立一个爬虫工程：\n\n                      scrapy startproject crawlspider\n\n\n这个我们很熟悉，接下来创建一个CrawlSpider爬虫\n\n    scrapy genspider -t crawl Crawlspider domain.com\n\n\n注意上面，我们比Spider爬虫建立时多了一个’-t crawl’,这是值爬虫的类 \n这样以后我们就可以在我们的spiders文件中找到这个爬虫\n\n\n\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nclass CrawlspiderSpider(CrawlSpider):\n    name = 'crawlspider'\n    allowed_domains = ['domain.com']\n    start_urls = ['http://domain.com/']\n\n    rules = (\n        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        i = {}\n        #i['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').extract()\n        #i['name'] = response.xpath('//div[@id=\"name\"]').extract()\n        #i['description'] = response.xpath('//div[@id=\"description\"]').extract()\n        return i\n\n上面是打开爬虫后，自动生成的一段代码，这段代码可以说基本上对于CrawlSpider爬虫的结构有一个构造，我们看到，有一个rules属性，还有这个爬虫继承的类是CrawlSpider，这两点就是与前几篇讲的Spider爬虫的区别。其中rules属性使这个爬虫的核心\n\n所以我们在开始讲我们的实战项目之前我们应该先讲一下这个rules属性。 \nrules属性由几个Rule对象构成，而Rule对象定义了提取链接等操作的规则\n\n那么Rule的构造又是怎样的呢？ \nRule对象有六个属性，他们分别是：\n\n\nLinkExtractor(…)，用于提取response中的链接\ncallback=‘str’，回调函数，对提取的链接使用，用于提取数据填充item\ncb_kwargs，传递给回调函数的参数字典\nfollow=True/False，对提取的链接是否需要跟进\nprocess_links，一个过滤链接的函数\nprocess_request，一个过滤链接Request的函数 \n上面的参数除了LinkExtractor外其它都是可选的，且当callback参数为None时，我们称这个rule为一个‘跳板’，也就是只下载页面，并不进行任何行为，通常作翻页功能\n\n\n我们需要解释主要是LinkExtractor参数和follow参数：\n\n一、LinkExtractor参数，明显是用来提取链接的。那么他是怎么来定义提取链接的规则的呢？它有十个参数，用来定义提取链接的规则，分别是： \n 1. allow=‘re_str’:正则表达式字符串，提取response中符合re表达式的链接。 \n 2. deny=‘re_str’：排除正则表达式匹配的链接 \n 3. restrict_xpaths=‘xpath_str’：提取满足xpath表达式的链接 \n 4. restrict_css=‘css_str’:提取满足css表达式的链接 \n 5. allow_domains=‘domain_str’:允许的域名 \n 6. deny_domains=‘domain_str’：排除的域名 \n 7. tags=‘tag’/[‘tag1’,’tag2’,…]：提取指定标签下的链接，默认会从a和area标签下提取链接 \n 8. attrs=[‘href’,’src’,…]：提取满足属性的链接 \n 9. unique=True/False：链接是否去重 \n 10.process_value：值处理函数，优先级要大于allow \n 以上的参数可以一起使用，以提取同时满足条件的链接\n\n二、follow参数： \n为Boolean值，用于是否跟进链接的处理，在callback为None时，默认是跟进链接的，值为True；当callback不为空时，默认是False的，不跟进链接。当然我们可以根据需要赋值，\n\n 那么，什么叫跟进，什么叫不跟进呢？\n 就是你前面定义的规则对于已经提取到的链接的页面是不是在进行一次提取链接。\n\n\n好！那么rules到底是怎么工作的呢？ \n这样的，对于Rule提取的链接会自动调用parse函数，并返回该链接的response，然后将这个response给callback回调函数，通过回调函数的解析对item进行填充\n\n对了，CrawlSpider爬虫还有一个parse_start_url()方法，用于解析start_urls中的链接页面，这个方法一般用于有跳板的爬虫中，用于对首页的解析\n\n说了那么多，我们来说说我们的爬虫项目。用CrawlSpider爬虫，爬取整站的小说\n\n我们的目标网站是：笔趣看小说网 \n这是一个盗版小说网站，只能在线观看，不能下载。 \n首页是这样的： \n \n\n\n整个首页的内容就是上面的样子，那我们的目标就是提取整个首页的所有的小说\n\n首先我们要创建一个CrawlSpider爬虫，创建步骤上面有。 \n先贴代码，再解释：\n\n\n\n# -*- coding: utf-8 -*-\nimport scrapy,re\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom crawlspider.items import CrawlspiderItem\n\n\nclass CrawlspidersSpider(CrawlSpider):\n    name = 'CrawlSpiders'\n    allowed_domains = ['sbiquge.com']\n    start_urls = ['https://www.sbiquge.com/biqukan/']\n\n    rules = (\n        Rule(LinkExtractor(allow=\"/\\d+?_\\d+?/\",unique=True),callback='parse_item',follow=False),\n        # 注意使用restricted_xpath提取链接的时候只能到标签就好了，千万不要到具体的标签属性，那叫画蛇添足。\n        # 这个rule实例有callback属性，那么它就是从所有下载到的response里面提取链接，并下载链接内容由回调函数处理数据\n    )\n    def parse_item(self, response):\n\n        chap_list = response.xpath('.//*[@class=\"listmain\"]/dl/dd')\n        for chapter in chap_list:\n            novel_name = chapter.xpath('//*[@id=\"book\"]/div[1]/div/a[2]/text()').extract_first()\n            chapter_name = chapter.xpath('./a/text()').extract_first()\n            chapter_link = chapter.xpath('./a/@href').extract_first()\n            if chapter_name:\n                item = CrawlspiderItem(chapter_title=chapter_name,novel_name=novel_name)\n                url = response.urljoin(chapter_link)\n                request = scrapy.Request(url=url,callback=self.parse_body)\n                request.meta['key'] = item\n                yield request\n\n    def parse_body(self,response):\n        item = response.meta['key']\n        content_list = response.xpath('.//*[@id=\"content\"]').re('([\\u4e00-\\u9fa5]|<br>)+?') # 匹配到的是一个列表\n        # 利用re直接匹配小说的汉字内容.正则可以匹配标签下的任何内容，这样我们可以提取我们想要的数据\n        content_str = ''.join(content_list)\n        content = re.sub('<br><br>','\\n  ',content_str)\n        # 对匹配的章节进行分段\n        item['content'] = content\n        yield item\n\n\n\n\n\n上面重点内容都是有注释的，如果有看不懂的地方，看一下我前面几篇文章，就可以了。 \n这样的经过上面我们填充的item中就有了小说的名字和小说的章节标题及内容了\n\n接下来就是如何将小说存进对应小说文件的，我们可以选择动态的创建文件的方式（不难的） \n在pipelines.py文件中\n\n\n\nclass CrawlspiderPipeline(object):\n    def process_item(self, item, spider):\n        novel = '{}.txt'.format(item['novel_name'])\n        # 动态创建小说的文件\n        self.file = open(novel, 'a')\n        self.file.write(item['chapter_title']+'\\n'+item['content'])\n        self.file.close()\n\n至于其他几个文件中的代码，和前面几篇的是一样的，有点基础的你应该是不用要的把！ \n这样的话，我们就可以爬取整个网站的小说了。这是我的成果： \n \n\n\n我只爬取了一点点就停下来了，另外稍微改写一下代码，就爬取到了图一所有小说的名字。 \n对了，如果你想让程序停下来就Ctrl+C哦！，两次呀。", "time": "2018_08_13_17_28_53", "link": "https://blog.csdn.net/killeri/article/details/80255500", "title": "scrapy进阶（CrawlSpider爬虫__爬取整站小说）"}
{"timestamp": "2018_08_13_17_28_53", "desc": "首先得解决环境和工具的问题 \nPython基础教程 \nPython3基础教程 \n大家也可以去慕课网看视频学习哦，关于选择Python2还是Python3的问题，上手的话还是直接选择3吧。\n\n\n\n关于爬虫\n\n爬虫就是在互联网中执行爬取有用信息的程序，总的工作流程如下: \n找到爬虫入口->获取目标链接->下载网页-> 解析网页 -> 获取价值信息 ->存库（文件保存）操作\n\n首先给自己一个伟大的小目标吧！或许明天的UC头条就是，震惊！一16岁编程奇才爬取某社区2亿条用户数据。\n\n\n\n开始吧\n\n我们的目标就从一个图片网站开始吧，坐好啦，老司机要发车了 –> mzitu.com\n\n\n\nhttp://www.mzitu.com/all 每日更新页面给了我们一个很好的爬虫入口，良心站长，F12进入浏览器调试模式，更方便开发人员观察\n\n\n\n\n\n找到入口和目标链接之后开始下载网页\n\n\n\n# -*- coding: UTF-8 -*-\nfrom urllib import request\n\n#目标抓取网页\nsrc = 'http://www.mzitu.com/all'\n#浏览器请求头（大部分网站没有这个请求头可能会报错）\nmheaders = {'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"}\n\n#读取一个网页\ndef getHtml(url):\n    req = request.Request(url,headers=mheaders) #添加headers避免服务器拒绝非浏览器访问\n    page = request.urlopen(req)\n    html = page.read()\n    return html.decode('utf-8')  # python3 python2版本直接返回html\n\nprint(getHtml(src))\n\n\n\nurllib.request.urlopen()请求你个网页，是不是so easy，各位看官，你老厉害了。\n\n拿到这个入口网页之后，我们顺着主线走，不要迷路了，那么入口网页里面用有的价值是什么了，当然是妹子，恩我们要奔着套路地址去，这里我们需要用到beautifulsoup4 ,当然高手一般都是使用正则表达式的，可是菜鸟真的合适吗，虽然bs4效率低一点，但是对开发人员友好啊。获取bs4方法,前提是你需要配置好python和pip（pip工具是python自带的）的环境变量，这里不赘述了\n\n\n  pip install beautifulsoup4\n\n\n好了，继续前行吧。去拿到我们的子目标，首先我们得明确子目标 \n\n\n\n\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom bs4 import BeautifulSoup\n\n#目标抓取网页\nsrc = 'http://www.mzitu.com/all'\n#浏览器请求头（大部分网站没有这个请求头可能会报错）\nmheaders = {'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"}\n\n#读取一个网页\ndef getHtml(url):\n    req = request.Request(url,headers=mheaders) #添加headers避免服务器拒绝非浏览器访问\n    page = request.urlopen(req)\n    html = page.read()\n    return html.decode('utf-8')  # python3 python2版本直接返回html\n\n#从入口爬取所有的目标链接\ndef getallUrl(html):\n    #构造一个bs对象\n    soup = BeautifulSoup(html, 'html.parser')\n    #使用bs对象寻找class为all的div 然后再寻找这些div里面的a标签，可能我们需要多试几次才能准确的get\n    all = soup.find('div',class_='all').find_all('a')\n    for li in all:\n        print(li)\n\ngetallUrl(getHtml(src))\n\n\n\n行了，我们又有目标了，遍历这些目标，最后在子目标里面寻找最终目标，对最终目标进行存库（文件操作）即可完成我们这次探险了！ \n \n最后一段旅程需要各位生手自行探索，老衲先行告退。对了，地图拿走不谢：\n\n\n\n# -*- coding: UTF-8 -*-\nfrom urllib import request\nfrom bs4 import BeautifulSoup\nimport uuid\nimport time\n\n#目标抓取网页\nsrc = 'http://www.mzitu.com/all'\n#浏览器请求头（大部分网站没有这个请求头可能会报错）\nmheaders = {'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"}\n\n#读取一个网页\ndef getHtml(url):\n    req = request.Request(url,headers=mheaders) #添加headers避免服务器拒绝非浏览器访问\n    page = request.urlopen(req)\n    html = page.read()\n    return html.decode('utf-8')  # python3 python2版本直接返回html\n\n#从入口爬取所有的目标链接\ndef getallUrl(html):\n    #构造一个bs对象\n    soup = BeautifulSoup(html, 'html.parser')\n    #使用bs对象寻找class为all的div 然后再寻找这些div里面的a标签，可能我们需要多试几次才能准确的get\n    all = soup.find('div',class_='all').find_all('a')\n    print(len(all))#无聊打印点什么\n    for li in all:\n        subSrc = li.attrs['href']\n        subHtml = getHtml(subSrc)\n        subSoup = BeautifulSoup(subHtml, 'html.parser')\n        page = subSoup.find('div', class_='pagenavi').find_all('span')\n        #page[-2]是表示数组从右(末端数2个) maxpage拿到套图最后一页\n        maxPage = page[-2].get_text()\n        i = 1\n        while (i <= int(maxPage)):\n            time.sleep(0.08) #休息0.08s，防止服务器拒绝频繁请求\n            tagetSrc = subSrc + '/' + str(i)\n            tagetHtml = getHtml(tagetSrc)\n            tagetSoup = BeautifulSoup(tagetHtml, 'html.parser')\n            img = tagetSoup.find('div', class_='main-image').find('img')\n            print(time.time())#无聊打印点什么\n            #uuid()构造一个世界唯一字符串，为了防止文件重名\n            name = img.attrs['alt'] + str(uuid.uuid4())\n            imgsrc = img.attrs['src']\n            print(imgsrc + \"-----\" + name)#无聊打印点什么\n            try:\n                #这里的指定存储路径，需要注意的是这里需手动创建文件夹，如需自动想、可以使用os库\n                request.urlretrieve(imgsrc, 'D:\\\\meizi\\\\' + '%s.jpg' % name)  # 指定目录位置\n            except BaseException:\n                #捕获异常情况\n                print('Error:there is something wrong!')\n                # 遇到IOError: [Errno socket error] [Errno 10060]服务器拒绝频繁访问 阻塞1s\n                time.sleep(1)\n                try:\n                    request.urlretrieve(imgsrc, 'D:\\\\meizi\\\\' + '%s.jpg' % name)  # 指定目录位置\n                except BaseException:\n                    print('Error:there is something wrong!over')\n            # print(tagetSrc)\n            i += 1\n        print('end')\n#开始\nprint('begin')\ngetallUrl(getHtml(src))\n#结束\nprint('over')\n\n\n成果 \n\n\n好了，这次旅行到此结束，对于这次初次旅程还满意吗？给各位老厉害的看官留几个宝箱吧： \n1.这种方式爬取数据存在什么弊端？该怎么完善？ \n2.有什么方法提高爬取效率？ \n3.反面思考我们的web网站怎么防止被机器爬取数据？ \n4.我们的爬虫（机器）面对验证码，登录等问题怎么处理？ \n…\n\n\n\n最后\n\n谢谢站长", "time": "2018_08_13_17_28_53", "link": "https://blog.csdn.net/xiaoping0915/article/details/62420139", "title": "【Python】从爬虫开始吧——爬取妹子图整站"}
{"timestamp": "2018_08_13_17_28_53", "desc": "今天写的是一篇关于ItemLoader的基本用法的一个点睛解释。 \n爬取网站：妹子图上所有的图片。链接可以打不开（直接用你的浏览器输入（www.meizitu.com）） \n首先贴几张图，是网站的首页和详情页的结构 \n \n首页是一个大图的形式呈现 \n \n详情页由很多小的图片组成的类似九宫格的形式\n\n我们的目标就是要爬取首页和详情页（15页）的全部图片\n\n当然，我们在这里肯定要用到我们的新知识，用ItemLoader进行数据的提取和对item的填充。\n\n爬虫类型：CrawlSpider爬虫 \n我们知道用CrawlSpider对网页的翻页是很方便的，直接定义Rule（不包含call参数）就可以实现自动化的翻页了，那么，我们的首页就要用parse_start_url方法进行提取了。\n\n\n\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.loader import ItemLoader\nfrom jj.items import JjItem\n\n\nclass CrawlspiderSpider(CrawlSpider):\n    name = 'crawlspider'\n    allowed_domains = ['meizitu.com']\n    start_urls = ['http://www.meizitu.com']\n\n    def parse_start_url(self, response):\n        # 用于提取首页的图片\n        Loader = ItemLoader(item=JjItem(),response=response)\n        Loader.add_xpath('picture_urls','.//*[@id=\"picture\"]/p/a/img/@src')\n        # 这样就图取出来了该网页所有的图片链接，也就是图片链接以一个列表的形式收集在一个字段里面\n        yield Loader.load_item()\n\n\n    rules = (\n        Rule(LinkExtractor(restrict_xpaths='.//*[@id=\"subcontent clearfix\"]/div[2]/span/a'),follow=False),\n        # 用于遍历所有的小标题如：颜值控，萌妹等\n        Rule(LinkExtractor(restrict_xpaths='.//*[@id=\"wp_page_numbers\"]/ul/li/a',unique=True),callback='parse_item', follow=False),\n        # 对上一个Rule下载下来的所有页面提取各个小标题的翻页链接（每个小标题好象有15页左右）\n    )\n    # 上面规则提取每个页面的其他页面的链接，并不进行跟进（因为一个页面上就能提取出所有详情页的链接，所以不需要进行跟进）\n    def parse_item(self, response):\n        Loader = ItemLoader(item=JjItem(), response=response)\n        Loader.add_xpath('picture_urls', './/*[@id=\"maincontent\"]/div/ul/li//a/img/@src')\n        # 将一个详情页的所有的图片链接都放到一个字段里面\n        print(Loader.get_collected_values('picture_urls'))\n        yield Loader.load_item()\n\n上面是我们的爬虫程序，很简单。所以用一个parse_start_url,爬取了首页上的所有的图片，并保存到了picture_urls字段中，也就是说，用一个字段保存了一个页面上所有的链接，然后自动调用图片的调度器和下载器对图片进行下载。 \n用parse_item方法对详情页的内容进行了提取，是同样的。在第二个方法里我用ItemLoader的get_collected_values方法获得了进入输入处理器之前的列表，算是做一个测试把。\n\n其他爬虫文件的配置都不难，你们可以通过学习我前面几篇文章的内容自己就可以做出来了。\n\n在这里我们明白了两点内容： \n一、item的一个字段不但接受一个str还可以接受一个列表（实际上，一个item字段可以接受任何python类型的数据，不只上面两个） \n二、图片链接字段只接受一个列表（这与一有点冲突，记住就好了。） \n在前面的内容，我还强调了说，图片链接字段只接受列表（即使只有一个图片），这里就讲明了原因，在某种程度上是为了ItemLoader服务的。\n\n这里和前面的item不一样的在于，前面几篇的item一个字段往往只接受了一个值，通过循环的方式每次返回一个值，而这里一个item字段接受了一个列表的数据。\n\n进过上面的程序的运行，我们的成果给祢看一下呀！ \n\n\n注意：本博客不涉黄！！！！！！！！！！！！！！！！！！ \n关注一波？\n\n这个程序只是对一个页面上的主体图片进行了提取，那 \n \n这些链接的内容该怎么提取呢，其实很简单。我给一个思路： \n增加一个Rule((allow=’http://www.meizitu.com/a/\\d+?.html’,unique=True),callback=”,follow=True) \n这样我们就能够提取到所有已经下载到的页面的如正则相匹配的链接并下载页面内容，然后在用回调函数解析页面就可以了！", "time": "2018_08_13_17_28_53", "link": "https://blog.csdn.net/killeri/article/details/80284049", "title": "scrapy进阶（爬取整站图片）——Itemloader作用机制点睛"}
{"timestamp": "2018_08_13_17_28_53", "desc": "scrapy的请求是并发进行的，但是我今天有一个需求是要顺序爬网站上的信息，爬的是搜狗热搜榜的电影、电视剧、动漫、综艺的热搜排行榜，每一个爬前三页。顺序爬取下来然后存到数据库中。 \n\n\n我的解决办法是在setting文件中将scrapy的并发数设置为1，当并发数为1的时候不就是同步了嘛\n\n\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\nCONCURRENT_REQUESTS = 1\n\n这个参数的默认值是16\n\n以下是我的代码：\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'\n}\n\n\nclass Topsougou_Spider(scrapy.Spider):\n    name = 'top_sougou_spider'\n\n    def start_requests(self):\n        reqs = []\n        # 综艺\n        for i in range(1, 4):\n            url = 'http://top.sogou.com/tvshow/all_' + str(i) + '.html'\n            req = scrapy.Request(url=url, headers=headers)\n            reqs.append(req)\n        # 电影\n        for i in range(1, 4):\n            url = 'http://top.sogou.com/movie/all_' + str(i) + '.html'\n            req = scrapy.Request(url=url, headers=headers)\n            reqs.append(req)\n        # 电视剧\n        for i in range(1, 4):\n            url = 'http://top.sogou.com/tvplay/all_' + str(i) + '.html'\n            req = scrapy.Request(url=url, headers=headers)\n            reqs.append(req)\n        # 动漫\n        for i in range(1, 4):\n            url = 'http://top.sogou.com/animation/all_' + str(i) + '.html'\n            req = scrapy.Request(url=url, headers=headers)\n            reqs.append(req)\n        return reqs\n\n    def parse(self, response):\n        item = TopSougouItem()\n        url = response.url\n        print url\n\n执行结果：", "time": "2018_08_13_17_28_53", "link": "https://blog.csdn.net/gpwner/article/details/78362052", "title": "python爬虫<将scrapy的请求设置为同步>"}
{"timestamp": "2018_08_13_17_28_53", "desc": "写博客只是为了分享一些我踩过的坑，有些坑真的很让人奔溃，所以写\n\n上一篇我们讲了用scrapy的原生管道下载图片，这篇来讲讲用自定义的管道来下载图片（当然也是在继承了原生管道后对方法的重定义）\n\n什么是scrapy的管道（pipelines）： \n讲了这么多我还没有具体讲一下什么是scrapy里面的管道，按这里我就具体讲一下什么是pipelines，pipelines有什么作用\n\n在一个工程里面，在pipelines.py文件中定义我们的管道，其实一个管道实际上就是一个类，而这个类定义了一些方法（属性），用来处理我们传进类（管道）中的数据，在处理完以后，再返回被处理以后的数据。那么，多个管道合用，当然就是讲一个数据先后传进多个管道中处理，最后输出数据了。\n\n下面我们实现的这个工程就实现了多个管道分别处理一个item\n\n实战项目：\n\n        用scrapy爬取豆瓣top250电影的影名，导演等信息还有海报（海报是一个图片，需要下载）\n        *很明显，这次的重点是图片和文件的内容的双重爬取，也就是我们要处理文本和图片，就需要用到我们的两个管道，图片处理管道MypipeimageslinePipeline和文本处理管道MyitemPipeline*\n\n\n这篇要结合前面的两篇来讲：\n\n爬取豆瓣文本的scrapy工程 \n用原生scrapy管道爬取图片\n\n首先我们用spider爬取出我们要的文本和图片的链接，这个我贴上我的代码，不过你最好自己去独立实现，这个不涉及很多，主要只是xpath和正则表达式的应用，具体的代码解释看第一篇文章，我会在这篇里面新增的代码进行注释\n\n\n\n# 大部分的代码解释在*爬取豆瓣文本的scrapy工程*，这里进行的是对新增代码的注释\n\nimport scrapy\nfrom mypipeline.items import MypipelineItem\nfrom scrapy.selector import Selector\n\n\nclass MypipespiderSpider(scrapy.Spider):\n    name = 'mypipespider'\n    allowed_domains = ['douban.com']\n\n    start_urls = [\n        'https://movie.douban.com/top250?start=0&filter='\n    ]\n\n    def parse(self, response):\n        lis = response.xpath('.//div[@class=\"article\"]/ol[@class=\"grid_view\"]/li')\n        for li in lis:\n            item = MypipelineItem()\n            title = li.xpath('.//div[@class=\"hd\"]/a/span/text()').extract_first()\n            director = li.xpath('.//div[@class=\"bd\"]/p[1]/text()').re('([\\u4e00-\\u9fa5]?·?[\\u4e00-\\u9fa5]+?)\\s')[0]\n            time_list = li.xpath('.//div[@class=\"bd\"]/p[1]/text()').re('\\d+?')\n            time = ''.join(time_list)\n            rate = li.xpath('.//span[@class=\"rating_num\"]/text()').extract_first()\n            quote = li.xpath('.//p[@class=\"quote\"]/span/text()').extract_first()\n            detail_link = li.xpath('.//div[@class=\"hd\"]/a/@href').extract_first()\n            picture_link = li.xpath('.//div[@class=\"pic\"]/a/img/@src').extract()\n            # 爬取图片的链接，注意：用数组的形式储存链接\n            if quote:\n                item['title'] = title\n                item['director'] = director\n                item['time'] = time\n                item['rate'] = rate\n                item['quote'] = quote\n                item['my_images_urls'] = picture_link\n                # 将链接储存到item里面\n            else:\n                item['title'] = title\n                item['director'] = director\n                item['time'] = time\n                item['rate'] = rate\n                item['my_images_urls'] = picture_link\n            request = scrapy.Request(url=detail_link, meta={\"key\": item}, callback=self.parse_detail)\n            yield request\n        next_page = response.xpath('//span[@class=\"next\"]/a/@href').extract_first()\n        if next_page is not None:\n            url = response.urljoin(next_page)\n            yield scrapy.Request(url, callback=self.parse)\n\n    def parse_detail(self, response):\n        item = response.meta['key']\n        sele = Selector(response)\n        short = sele.xpath('.//div[@id=\"link-report\"]/span[1]/text()').extract_first()\n        item['short'] = short\n        yield item\n\n用原生scrapy爬取图片这篇讲过当图片链接存在特定的键后，启动爬虫会自动调用scrapy的调度器和下载器对图片链接指向的图片进行下载并存在相应的文件夹\n\n接下来来讲一下pipelines.py文件中的管道（类） \n要想定义自己的imagespipeline，那首先要继承原生的imagespipeline\n\n\n\nfrom scrapy.pipelines.images import ImagesPipeline\n\nclass MypipeimageslinePipeline(ImagesPipeline):\n    #继承类\n    def get_media_requests(self, item, info):\n        for images_url in item['my_images_urls']:\n            yield scrapy.Request(images_url)\n            # 返回图片链接所指向的response\n    def item_completed(self, results, item, info):\n        images_path = [x['path'] for ok,x in results if ok]\n        # results是之前get_media_requests返回的，具体看下面\n        if not images_path:\n            raise DropItem(\"item contains no images\")\n        item['image_path'] = images_path\n        # 将图片的链接存储在item里面\n        return item\n        # return item 必不可少，用来返回item以便其它管道调用item进行处理\n\n上面的results是由get_media_requests方法返回的一个两个元素的元组列表，每个元组里面包含了一个布尔值和一个字典。实例如下：\n\n    [(True,\n        {'checksum':'图片的MD5hash',\n        'url':'图片的链接',\n        'path':'图片的储存路径',}\n        ),\n        ...\n    (False,\n        Failure())]\n\n\n上面图片下载成功就是True，失败就是False。\n\n\n\nimages_path = [x['path'] for ok,x in results if ok]\n# 这一句就是用来提取图片的存储路径的\n\n以上我们自定义了imagepipeline，这个管道用来处理图片，接下来我们定义文本处理的管道\n\n\n\nclass MyitemPipeline(object):\n    def __init__(self):\n        self.file = open('movie_and_details.csv','wb')\n# 将文本存储到文件中\n    def process_item(self, item, spider):\n        self.file.write(bytes(str(item),encoding='utf-8'))\n        # 这里的bytes是python3可以用的，如果是python2.7等出现问题你可以结合json模块进行数据的序列化，然后存储就可以了。\n        return item\n\n\n上面我们定义了两个管道，接下来我们就考虑如何调用这两个管道了。\n\n在settings.py里面我们要更改一些设置。除了上篇讲到的设置外还有\n\n\n\nITEM_PIPELINES = {\n    'mypipeline.pipelines.MypipeimageslinePipeline':1,\n   'mypipeline.pipelines.MyitemPipeline': 300\n}\n\n数字越小，管道的优先级越高，优先调用。数字控制在0~1000. \n其余的设置请参考上一篇文章。\n\n至于items.py文件就是定义item\n\n\n\nimport scrapy\n# 直接导入*from scrapy import Field*会更方便哦\n\nclass MypipelineItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    quote = scrapy.Field()\n    director = scrapy.Field()\n    time = scrapy.Field()\n    rate = scrapy.Field()\n    title = scrapy.Field()\n    short = scrapy.Field()\n    my_images_urls = scrapy.Field()\n    my_images = scrapy.Field()\n    image_path = scrapy.Field()\n\n\n那么，这就是，整个文章的内容。有什么不懂的，欢迎询问，还有，开源项目，欢迎留言指导。", "time": "2018_08_13_17_28_53", "link": "https://blog.csdn.net/killeri/article/details/80228089", "title": "scrapy初探（自制管道和多个管道合用）"}
{"timestamp": "2018_08_13_17_28_53", "desc": "拉格朗日插值数学原理：\n\n根据数学知识，对于平面上已知的n个点（无两点在一条直线上）可以找到一个 n-1 次多项式： \ny=a0+a1x+a2x2+...+an−1xn−1y=a_0 +a_1x+ a_2x^2+ ... + a_{n-1}x^{n-1}\n\n为了进行根据新的x， 求出对应的 y值，需要求出上式中的系数 a0,a1,a2....an−1a_0, a_1, a_2.... a_{n-1}\n\n因为n个点(x1,y1),(x2,y2)...(xn,yn)(x_1,y_1), (x_2,y_2) ...(x_n,y_n)在以上多项式上，代入每个点，得 \ny1=a0+a1x1+a2x21+...+an−1xn−11y_1 = a_0 + a_1x_1 + a_2x_1^2 + ... + a_{n-1}x_1^{n-1} \ny2=a0+a1x2+a2x22+...+an−1xn−12y_2 = a_0 + a_1x_2 + a_2x_2^2 + ... + a_{n-1}x_2^{n-1} \n............ \nyn=a0+a1xn+a2x2n+...+an−1xn−1ny_n = a_0 + a_1x_n + a_2x_n^2 + ... + a_{n-1}x_n^{n-1}\n\n以上方程组中，n个方程，n个未知数a0,a1...an−1a_0, a_1 ...a_{n-1},所以方程可解，可以利用线性代数中的行列式求解。 \n解出的拉格朗日插值多项式为： \nL(x)=y1(x−x2)(x−x3)...(x−xn)(x1−x2)(x1−x3)...(x1−xn)+y2(x−x1)(x−x3)...(x−xn)(x2−x1)(x2−x3)...(x2−xn)+......L(x) = y_1\\frac{(x-x_2)(x-x_3)...(x-x_n)}{(x_1-x_2)(x_1-x_3)...(x1-x_n)}\n+y_2\\frac{(x-x_1)(x-x_3)...(x-x_n)}{(x_2-x_1)(x_2-x_3)...(x2-x_n)} + ......  \n+yn(x−x1)(x−x2)...(x−xn−1)(xn−x2)(xn−x3)...(xn−xn−1)+ y_n\\frac{(x-x_1)(x-x_2)...(x-x_{n-1})}{(x_n-x_2)(x_n-x_3)...(x_n-x_{n-1})} \n=∑i=0nyi∏j=0,j≠ix−xixi−xj=\\sum_{i=0}^n y_i \\prod_{j=0,j\\neq i} \\frac{x-x_i}{x_i -x_j}\n\n实例： \n假设一个2次多项行式ff, 按照前面的介绍，取三个点为 \nf(3)=10,f(6)=8,f(9)=4f(3)=10, f(6) = 8, f(9)=4\n\nl0(3)=(x−6)(x−9)(3−6)(3−9)l_0(3)= \\frac{(x-6)(x-9)}{(3-6)(3-9)} \nl1(6)=(x−3)(x−9)(6−3)(6−9)l_1(6)= \\frac{(x-3)(x-9)}{(6-3)(6-9)} \nl2(9)=(x−3)(x−6)(9−3)(9−6)l_2(9)= \\frac{(x-3)(x-6)}{(9-3)(9-6)}\n\nL(x)=f(3)l0(3)+f(6)l1(6)+f(9)l2(9)L(x)=f(3)l_0(3)+f(6)l_1(6)+f(9)l_2(9) \n=10(x−6)(x−9)(3−6)(3−9)+8(x−3)(x−9)(6−3)(6−9)+4(x−3)(x−6)(9−3)(9−6)=10 \\frac{(x-6)(x-9)}{(3-6)(3-9)}+8 \\frac{(x-3)(x-9)}{(6-3)(6-9)}+4 \\frac{(x-3)(x-6)}{(9-3)(9-6)} \n=−x2+3x+909=\\frac{-x^2+3x+90}{9}\n\n插值f(10)=209f(10)=\\frac{20}{9}\n\npython 实现\n\ny=a0+a1x+a2x2y=a_0 +a_1x+ a_2x^2\n\n\n\nfrom scipy.interpolate  import lagrange\nx = [3, 6, 9]\ny = [10, 8, 4]\nlagrange(x,y)\n#poly1d([ -0.11111111,   0.33333333,  10.        ])\n\n以上 lagrange(x,y)lagrange(x,y) 的输出值 poly1d([−0.11111111,0.33333333,10.])poly1d([ -0.11111111,   0.33333333,  10.        ]) 值的是多项式的三个系数 \n即： a0=−0.11111,a1=0.3333333,a2=10.a_0= -0.11111 , a_1 = 0.3333333,  a_2=10. \n\n如果要进行插值操作，可以：\n\n\n\nlagrange(x, y)(10)\n# 2.222222", "time": "2018_08_13_17_28_53", "link": "https://blog.csdn.net/a1368783069/article/details/51491749", "title": "拉格朗日插值 python  scipy"}
{"timestamp": "2018_08_13_17_28_54", "desc": "其实很多编程语言都可以做爬虫，例如java、c#、php等等甚至excel都可以抓网页的图表，那么为什么我们要用Python呢？它简单、便捷，而且有好多库可以选择，可以说python是写爬虫的首选了！\n\n今天就来带大家写一个简单而又完整的爬虫，我们来抓取整站的图片的，并且保存到电脑上！\n\n\n\n\n\n准备工作\n\n工具：Python3.6、pycharm\n\n库：requests、re、time、random、os\n\n目标网站：妹子图（具体url大家自己去代码里看。。。）\n\n\n\n\n\n在写代码之前\n\n在我们开始写代码之前，要先对网站进行分析，重点有这个几个地方：\n\n1、先判断网页是否静态网页，这个关系我们采用的爬虫手段！\n\n简单的说，网页中的内容，在网页源代码中都可以找到，那么就可以断定，这个网站是静态的了；如果没有找到，就需要去开发者工具中查找，看看是抓包呢还是分析js结构或者其他的方式。\n\n2、看看网页的结构，大致清楚抓取目标数据，需要几层循环，每次循环的方式，以及是否保证没有遗漏！\n\n3、根据网页源代码来决定采用的匹配方式\n\n一般来说，正则表达式是处理字符串最快的方式，但是在爬虫中它的效率并不是很高，因为它需要遍历整个html来匹配相关内容，如果网页源代码比较规整的话，建议采用bs4或者xpath等等解析网页结构的方式比较好！\n\n当然，今天我们是基础向的爬虫，就用正则表达式了，毕竟正则是必须掌握的内容！\n\n那么，具体怎么写爬虫代码呢~？简单的举例给大家说下：\n\n如果是手工操作的话，大概是这个流程\n\n打开主页==>选择一个分类==>选择一个图集==>依次选择图片==>右键保存==>重复以上保存其他图片\n\n那么这个过程放到代码中呢，它的结构大概是这样：\n\n访问主页url==>找到并循环所有分类==>创建分类文件夹==>访问分类url==>找到页码构建循环分类所有页==>循环页面所有图集==>创建图集文件夹==>找到图集内所有图片url==>保存到对应文件夹\n\n好了，思路也有了，那就废话不多说了，我们来写代码吧~！\n\n\n\n\n\n开始写代码\n\n首先是导入上述的各种库，没有的需要安装一下！然后写入以下几行代码获取网页源代码看看是否有反爬：\n\n\n\nimport requests\nimport time\nimport random\nimport re\nimport os\n\n\nurl = 'http://www.meizitu.com/'\nhtml = requests.get(url)\nhtml.encoding = 'gb2312'\n\n\n如果能顺利打印出源代码且和网页右键查看的源代码一致，那么可以判定该网站基本没有反爬了！\n\n第16行代码的含义是给html设定编码格式。因为Python3默认是utf-8,如果网站不是这个编码格式的话，会出现乱码，所以我们直接指定一下。\n\n接下来呢，就是找到所有分类的名字和url了，来看看网页中和源代码中，它的位置在哪\n\n\n\n\n\n全部在a标签的属性中，那么我们可以用一行代码获取了\n\n\n\ninfos = re.findall(r'a href=\"(http://www.meizitu.com/.*?html)\" target=\"_blank\" title=\"(.*?)\" ',html.text)\n\n这里用正则匹配，2个括号中的内容就是我们需要的url和名字了，然后开始构建循环遍历所有的分类\n\n\n\n上一步取出的infos是列表，而且每一个元素都是一个元组，格式为（url，名字），所有我们用2个元素去遍历infos，来获取我们需要的内容，先打印下看看结果是否正确！\n\n这里先不创建文件夹，先进行下一步，访问分类的url，然后开始构建分类中的页码吧！分析网页发现，所有的页码都在下方，但是还是稍有不同：没有当前页、多了下一页和末页\n\n\n\n\n\n由于存在图集不足一页的情况（上述源代码就不会出现），所以我们这么处理循环\n\n\n\n19-21行获取分类的源代码，22行获取所有页码的url，然后用set（）函数去重，再新建一个空列表，将分类的url加进去，注意，元组是不能用append（）方法添加到列表中的，所以要先将set元组转化为列表然后分别重新拼接列表内所有的url，在将2个列表相加的方式合并为一个列表！这样我们就得到了分类下所有翻页页面的url\n\n\n\n循环所有的url，获取所有图集的url列表，27行没有用encoding指定编码是因为这里我不需要取到中文的内容，所以简写了一下！终于该取图片了！\n\n\n\n图集的title和图集内所有图片的url都取到了！其实到这里就已经完成了爬虫的80%了！剩下的20%就是保存图片到本地，这里就不多说了，给大家提供2个代码片段，一个是新建文件夹并判断是否存在，一个是剔除字符串内不符合命名要求的字符\n\n\n\npath = 'E://python/mn/meizitu/%s/'%sor#路径\nif os.path.exists(path):#判断路径及文件夹是否存在，不存在即创建\n    pass\nelse:\n    os.mkdir(path)\n\n\n\n\ndef new_title(title):\n    rstr = r\"[\\/\\\\\\:\\*\\?\\\"\\<\\>\\|]\"  # '/ \\ : * ? \" < > |'\n    new_title = re.sub(rstr, \"_\", title)  # 替换为下划线\n    return new_title\n\n\n\n最终完整代码和运行效果\n\n\n\n在请求中加入了时间模块的暂停功能，不加入的话可能会被网页拒绝访问！\n\n在最后请求图片地址的时候，需要加入UA来告诉服务器你是浏览器而不是脚本，这个是最常用的反爬手段了\n\n\n\n#author:云飞\n#QQ群542110741\nimport requests\nimport time\nimport random\nimport re\nimport os\n\ndef new_title(title):\n    rstr = r\"[\\/\\\\\\:\\*\\?\\\"\\<\\>\\|]\"  # '/ \\ : * ? \" < > |'\n    new_title = re.sub(rstr, \"_\", title)  # 替换为下划线\n    return new_title\n\nurl = 'http://www.meizitu.com/'\nhtml = requests.get(url)\nhtml.encoding = 'gb2312'\ninfos = re.findall(r'a href=\"(http://www.meizitu.com/.*?html)\"  target=\"_blank\" title=\"(.*?)\" ',html.text)\ni = 1\nfor sor_url,sor in infos:\n    sor = new_title(sor)\n    path = 'E://python/mn/meizitu/%s/'%sor#路径\n    if os.path.exists(path):#判断路径及文件夹是否存在，不存在即创建\n        pass\n    else:\n        os.mkdir(path)\n    time.sleep(random.random())\n    sor_html = requests.get(sor_url)\n    sor_html.encoding = 'gb2312'\n    atlas = set(re.findall(r\"<li><a href='(.*?html)'>\\d+</a></li>\",sor_html.text))\n    atlas_lis = []\n    atlas_lis.append(sor_url)\n    atlas_lis += [url+'a/'+x for x in list(atlas)]\n    for atla in atlas_lis:\n        atla_html = requests.get(atla).text\n        at_url_lis = re.findall(r'h3 class=\"tit\"><a href=\"(http://www.meizitu.com/.*?html)\"  targe',atla_html)\n        for at_url in at_url_lis:\n            at_html = requests.get(at_url)\n            at_html.encoding = \"gb2312\"\n            atlas_title = ''.join(re.findall(r'<title>(.*?)</title>',at_html.text))\n            atlas_title = new_title(atlas_title)\n            img_path = 'E://python/mn/meizitu/%s/%s/'%(sor,atlas_title)\n            if os.path.exists(img_path):#判断路径及文件夹是否存在，不存在即创建\n                pass\n            else:\n                os.mkdir(img_path)\n            img_urls = re.findall(r'src=\"(http://mm.chinasareview.com/.*?jpg)\" /><br />',at_html.text)\n            k = 1\n            for img_url in img_urls:\n                header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:57.0) Gecko/20100101 Firefox/57.0'}\n                data = requests.get(img_url,headers=header).content#获取图片的二进制格式\n                with open('%s%s'%(img_path,img_url.split('/')[-1]),'wb') as f:\n                    f.write(data)\n                print(\"【正在下载】 {%s}的第%d张图片，一共下载了%d张图片\"%(atlas_title,k,i))\n                i += 1\n                k += 1\n\n\n\n\n下载一段时间后的效果", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/programmer_yf/article/details/81043412", "title": "Python老司机手把手带你写爬虫，整站下载妹子图，一次爽个够！"}
{"timestamp": "2018_08_13_17_28_54", "desc": "0.首先还是按照惯例，上效果图（no picture, no …） \n \n可以看到使用ajax异步的方式去做crud操作时，整个页面是没有刷新的（参考左上角的刷新按钮），这种体验更像是C/S架构\n\n1.AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML）——用于创建更好更快以及 交互性 更强的Web应用程序的技术。\n\n2.页面代码 emp_list.jsp\n\n\n\n<%@ page language=\"java\" contentType=\"text/html; charset=utf-8\"\n    pageEncoding=\"utf-8\"%>\n<%@ taglib prefix=\"s\" uri=\"/struts-tags\" %>\n<!DOCTYPE html PUBLIC >\n<html>\n    <head>\n        <title>ajax emp system</title>\n        <meta charset=\"UTF-8\" />\n    </head>\n    <body>\n        <div class=\"container\">\n            <!-- 模态框（Modal）添加框 -->\n            <div></div>\n            <div class=\"modal fade\" id=\"modalAddEmp\" tabindex=\"-1\" role=\"dialog\" aria-labelledby=\"myModalLabel\" aria-hidden=\"true\">\n                <div class=\"modal-dialog\">\n                    <div class=\"modal-content\">\n                        <div class=\"modal-header\">\n                            <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">\n                    &times;\n                </button>\n                            <h4 class=\"modal-title\" id=\"myModalLabel\">\n                    添加员工\n                </h4>\n                        </div>\n                        <div class=\"modal-body\">\n                            <form id=\"form_add\" class=\"form-horizontal\" role=\"form\">\n                                <div class=\"form-group\">\n                                    <label for=\"firstname\" class=\"col-sm-2 control-label\">名字</label>\n                                    <div class=\"col-sm-10\">\n                                        <input type=\"text\" class=\"form-control\" name=\"emp.ename\" placeholder=\"请输入员工姓名\">\n                                    </div>\n                                </div>\n                                <div class=\"form-group\">\n                                    <label for=\"lastname\" class=\"col-sm-2 control-label\">职业</label>\n                                    <div class=\"col-sm-10\">\n                                        <input type=\"text\" class=\"form-control\" name=\"emp.job\" placeholder=\"请输入员工职业\">\n                                    </div>\n                                </div>\n                                <div class=\"form-group\">\n                                    <label for=\"lastname\" class=\"col-sm-2 control-label\">薪水</label>\n                                    <div class=\"col-sm-10\">\n                                        <input type=\"text\" class=\"form-control\" name=\"emp.sal\" placeholder=\"请输入员工薪水\">\n                                    </div>\n                                </div>\n                                <div class=\"form-group\">\n                                    <label for=\"lastname\" class=\"col-sm-2 control-label\">选择部门</label>\n                                    <div class=\"col-sm-10\">\n                                        <select id=\"addSele\" class=\"form-control\" name=\"emp.dept.deptno\">\n\n                                        </select>\n                                    </div>\n                                </div>\n                                <div class=\"form-group\">\n                                    <div class=\"col-sm-offset-2 col-sm-10\">\n                                        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">关闭</button>\n            <button id=\"btnadd\" type=\"button\" class=\"btn btn-primary\" data-dismiss=\"modal\">确认添加</button>\n                                    </div>\n                                </div>\n                            </form>\n                        </div>\n\n                    </div>\n                    <!-- /.modal-content -->\n                </div>\n                <!-- /.modal -->\n            </div>\n\n            <!-- 模态框（Modal）修改框 -->\n            <div>\n                <div   class=\"modal fade\" id=\"modalUpdateEmp\" tabindex=\"-2\" role=\"dialog\" aria-labelledby=\"myModalLabel\" aria-hidden=\"true\">\n                    <div class=\"modal-dialog\">\n                        <div class=\"modal-content\">\n                            <div class=\"modal-header\">\n                                <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">\n                    &times;\n                </button>\n                                <h4 class=\"modal-title\" id=\"myModalLabel\">\n                    修改员工\n                </h4>\n                            </div>\n                            <div class=\"modal-body\">\n                                <form id=\"form_update\" class=\"form-horizontal\" role=\"form\">\n                                    <input type=\"text\" id =\"empno\" name=\"emp.empno\" \"\n                  hidden=\"hidden\">\n                                    <div class=\"form-group\">\n                                        <label for=\"firstname\" class=\"col-sm-2 control-label\">名字</label>\n                                        <div class=\"col-sm-10\">\n                                            <input type=\"text\" id=\"ename\" class=\"form-control\" name=\"emp.ename\" placeholder=\"请输入员工姓名\">\n                                        </div>\n                                    </div>\n                                    <div class=\"form-group\">\n                                        <label for=\"lastname\" class=\"col-sm-2 control-label\">职业</label>\n                                        <div class=\"col-sm-10\">\n                                            <input type=\"text\" id=\"job\" class=\"form-control\" name=\"emp.job\" placeholder=\"请输入员工职业\">\n                                        </div>\n                                    </div>\n                                    <div class=\"form-group\">\n                                        <label for=\"lastname\" class=\"col-sm-2 control-label\">薪水</label>\n                                        <div class=\"col-sm-10\">\n                                            <input type=\"text\" id=\"sal\" class=\"form-control\" name=\"emp.sal\" placeholder=\"请输入员工薪水\">\n                                        </div>\n                                    </div>\n                                    <div class=\"form-group\">\n                                        <label for=\"lastname\" class=\"col-sm-2 control-label\">选择部门</label>\n                                        <div class=\"col-sm-10\">\n                                            <select id=\"updateSele\" class=\"form-control\" name=\"emp.dept.deptno\">\n                                                <s:iterator value=\"listDept\" var=\"dept\">\n                                                    <option value=\"<s:property value=\" #dept.deptno \"/>\"><s:property value=\"#dept.dname\" /></option>\n                                                </s:iterator>\n                                            </select>\n                                        </div>\n                                    </div>\n                                    <div class=\"form-group\">\n                                        <div class=\"col-sm-offset-2 col-sm-10\">\n                                            <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">关闭</button>\n            <button id=\"btnupdate\" type=\"button\" class=\"btn btn-primary\" data-dismiss=\"modal\">确认修改</button>\n                                        </div>\n                                    </div>\n                                </form>\n                            </div>\n\n                        </div>\n                        <!-- /.modal-content -->\n                    </div>\n                    <!-- /.modal -->\n                </div>\n            </div>\n            <!--\n        ${pageContext.request.contextPath}/emp_toadd\n    -->\n            <a id=\"btnAdd\" class=\"btn btn-default btn-lg \" data-toggle=\"modal\" data-target=\"#modalAddEmp\">添加员工</a>\n            <table class=\"table table-hover\" id=\"tbl\">\n                <caption>&nbsp&nbsp员工信息列表</caption>\n                <thead>\n                    <tr>\n                        <th>员工编号</th>\n                        <th>员工姓名</th>\n                        <th>职业</th>\n                        <th>部门</th>\n                        <th>工资</th>\n                        <th>操作</th>\n                    </tr>\n                </thead>\n                <tbody id=\"tbody\">\n\n                </tbody>\n            </table>\n        </div>\n\n        <!-- 新 Bootstrap 核心 CSS 文件 -->\n        <link href=\"http://cdn.static.runoob.com/libs/bootstrap/3.3.7/css/bootstrap.min.css\" rel=\"stylesheet\">\n        <!-- jQuery文件。务必在bootstrap.min.js 之前引入 -->\n        <script src=\"http://cdn.static.runoob.com/libs/jquery/2.1.1/jquery.min.js\"></script>\n        <!-- 最新的 Bootstrap 核心 JavaScript 文件 -->\n        <script src=\"http://cdn.static.runoob.com/libs/bootstrap/3.3.7/js/bootstrap.min.js\"></script>\n        <script type=\"text/javascript\">\n            var url_delemp = '${pageContext.request.contextPath}/emp_delAjax?emp.empno';\n            var url_getemp = '${pageContext.request.contextPath}/emp_getAjax?emp.empno';\n            var url_addemp = '${pageContext.request.contextPath}/emp_addAjax';\n            var url_updateemp = '${pageContext.request.contextPath}/emp_updateAjax';\n            var url_getemplist = '${pageContext.request.contextPath}/emp_listAjax';\n            var url_getdeptlist = '${pageContext.request.contextPath}/dept_listAjax';\n            $(function() {\n                refreshEmpList();\n                bindEven();\n            });\n            window.onload=function(){ \n                getDeptList();\n            }\n            //获取部门列表\n            function getDeptList(){\n                $.getJSON(url_getdeptlist,function(data,status){\n                    $adds = $('#addSele');\n                    $upds = $('#updateSele');\n                    $adds.html('');\n                    $upds.html('');\n                    $.each(data.list, function(index,d) {\n                        $adds.append('<option value =\"'+ d.deptno + '\">' + d.dname +\n                                '</option>');\n                        $upds.append('<option value =\"'+ d.deptno + '\">' + d.dname +\n                                '</option>');\n                    });\n                });\n            }\n            //绑定事件\n            function bindEven() {\n                $('table').on('click', '#btnDel', function() {\n                    if(confirm('真的要删除吗？')) {\n                        //异步删除\n                        delEmp($(this).attr('value'));\n                    }\n                });\n                $('table').on('click', '#btnModif', function() {\n                    //先show出修改的模态层，并将查到的数据加进去\n                    $tr = $(this).parent().parent();\n                    var $value = $tr.children('td').eq(3).attr('deptno');\n                    $('#updateSele').val($value);\n                    $('#empno').val($tr.children('td').eq(0).text());\n                    $('#ename').val($tr.children('td').eq(1).text());\n                    $('#job').val($tr.children('td').eq(2).text());\n                    $('#sal').val($tr.children('td').eq(4).text());\n\n                    console.log($('#updateSele').val());\n                });\n\n                $('#btnadd').on('click', function() {\n                    $res = $('#form_add').serialize();\n                    $.ajax({\n                        url: url_addemp,\n                        type: \"POST\", //GET或POST,\n                        data: $res,\n                        success: function(data) {\n                            refreshEmpList();\n                        }\n                    });\n                });\n\n                $('#btnupdate').on('click', function() {\n                    $res = $('#form_update').serialize();\n                    $.ajax({\n                        url: url_updateemp,\n                        type: 'post',\n                        data: $res,\n                        success: function(data) {\n                            refreshEmpList();\n                        }\n\n                    });\n                });\n            }\n            //删除员工\n            function delEmp(empno) {\n                console.log('empno:' + empno);\n                $.getJSON(url_delemp + \"=\" + empno, function(data, status) {\n                    console.log(data);\n                    //刷新列表\n                    refreshEmpList();\n                });\n            }\n            //刷新列表\n            function refreshEmpList() {\n                //异步刷新\n                $.getJSON(url_getemplist, function(data, status) {\n                    console.log(data);\n                    //刷新列表\n                    $tb = $('#tbody');\n                    $tb.html('');\n                    $.each(data.list, function(index,data) {\n                        $tb.append('<tr><td>'+data.empno+'</td>'+\n                        '<td>'+data.ename+'</td>'+\n                        '<td>'+data.job+'</td>'+\n                        '<td deptno ='+data.dept.deptno+'>'+data.dept.dname+'</td>'+\n                        '<td>'+data.sal+'</td>'+\n                        '<td><a id=\"btnDel\" value=\"'+data.empno+'\">删除</a>  &nbsp&nbsp&nbsp&nbsp  <a id=\"btnModif\" data-toggle=\"modal\" data-target=\"#modalUpdateEmp\">修改</a></td></tr>');\n                    });\n                });\n            }\n        </script>\n    </body>\n</html>\n\n3.服务器数据支持代码 EmpAction.java\n\n\n\n@Controller\n@Scope(\"prototype\")\n@ParentPackage(\"json-default\")\npublic class EmpAction extends ActionSupport{\n    private Emp emp;\n    private List<Emp> list;\n    @Autowired EmpService empService;\n\n    @Action(value= \"/emp_listAjax\",results={\n            @Result(name=\"success\",type=\"json\", params = { \"includeProperties\",\n            \"^list\\\\[\\\\d+\\\\]\\\\.empno,^list\\\\[\\\\d+\\\\]\\\\.ename,^list\\\\[\\\\d+\\\\]\\\\.job,^list\\\\[\\\\d+\\\\]\\\\.hiredate,^list\\\\[\\\\d+\\\\]\\\\.sal,^list\\\\[\\\\d+\\\\]\\\\.dept,^list\\\\[\\\\d+\\\\]\\\\.dept.dname,^list\\\\[\\\\d+\\\\]\\\\.dept.deptno\" })\n    })\n    public String listAjax(){\n        list = empService.getAll();\n        return SUCCESS;\n    }\n\n    @Action(value = \"/emp_delAjax\",results={\n        @Result(name = \"success\",type =\"json\",params = { \"includeProperties\",\"^emp.empno,^emp.ename\" })\n    })\n    public String delAjax(){\n        emp = empService.find(emp);\n        empService.del(emp);\n        return SUCCESS;\n    }\n\n    @Action(value = \"/emp_addAjax\")\n    public String addAjax(){\n        empService.add(emp);\n        return null;\n    }\n\n    @Action(value = \"/emp_updateAjax\")\n    public String updateAjax(){\n        empService.update(emp);\n        return null;\n    }\n}\n\nDeptAction.java\n\n\n\n@Controller\n@Scope(\"prototype\")\n@ParentPackage(\"json-default\")\npublic class DeptAction extends ActionSupport{\n    private List<Dept> list;\n    @Autowired DeptService deptService;\n\n    @Action(value= \"/dept_listAjax\",results={\n        @Result(name=\"success\",type=\"json\")\n    })\n    public String listAjax(){\n        list = deptService.getAll();\n        return \"success\";\n    }\n}\n\n4.解决的问题 \n（a）bootstrap 多个（大于2个）模态框 在页面上谁都显示不出来问题 \n这种情况只需要在模态框的div外层各套一个div即可 \n（b）在struts2的Action中使用JSON数据格式来传值 \n首先我们的Action类的父包配置为json-default，即在Action类头部注解上@ParentPackage(“json-default”) 然后指定@Result 的type属性为json 最后使用正则的方式表明你想要获得的数据 eg: params = { “includeProperties”,”^emp.empno,^emp.ename” }\n\n5.附录一些常用的jQuery代码 \n(a)AJAX请求\n\n$(function() {\n    $('#btnSend').click(function() {\n        $.ajax({\n            type: \"GET\", //GET或POST,\n            async:true, //默认设置为true，所有请求均为异步请求。\n            url: \"${pageContext.request.contextPath}/dept_listAjax\",\n            data: {\n                username: $(\"#username\").val(),\n                content: $(\"#content\").val()\n            },\n            dataType: \"json\", //xml、html、script、jsonp、text\n            beforeSend:function(){},\n            complete:function(){},\n            success: function(data) {\n                alert(data)\n            }\n            error:function(){},\n        });\n    });\n});\n\n(b)获取checkbox，并判断是否选中\n\n\n\n$(\"input[type='checkbox']\").is(':checked') \n//返回结果：选中=true，未选中=false\n\n(c)获取checkbox选中的值\n\n\n\nvar chk_value =[]; \n$('input[name=\"test\"]:checked').each(function(){ \n    chk_value.push($(this).val()); \n});\n\n6.Demo下载", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/xiaoping0915/article/details/59735713", "title": "【J2EE】ajax实现页面无刷新完成crud操作"}
{"timestamp": "2018_08_13_17_28_54", "desc": "还记得上次的发车（探索）吗？小伙伴有木有出现爬虫程序出现异常停止吗？上次没上车的童靴可以点击传送门\n\n\n\n那么问题来了\n\n为啥我们的爬虫程序会中途死亡？那么我们需要了解的是如何反制爬虫？ \n知道了他们的防守方式才能使我们的进攻更为有效！ \n通常很多网站不允许非浏览器访问，还有一个 ip 频繁访问会短暂禁止该IP访问。所以我们需要做的事情就是将自己的爬虫程序伪装为浏览器访问并且在 ip 被禁的时候使用代理 ip 来协助访问。\n\n\n\n准备工作\n\n使用 pip 工具下载 beautiflsoup4 、requests、lxml\n\n\n  pip install beautiflsoup4  \n  pip install requests \n  pip install lxml\n\n\n话不多说，直接来看我们的代码，这次我们换一个网址来测试（meizitu.com）：\n\n# -*- coding: UTF-8 -*-\nimport requests\nimport random\nimport time\nfrom bs4 import BeautifulSoup\n\n#伪装浏览器头\nuser_agent_list = [\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n    \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\n    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\n    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\n    \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\n    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n]\n\n##初始化一个list用来存放我们获取到的IP,当然我们也可以通过在代理ip服务商哪里去获取IP\niplist = ['60.207.239.245:3128', '124.88.67.24:843', '210.101.131.231:8080', '124.192.106.247:3128', '202.106.16.36:3128', '220.248.229.45:3128', '61.185.137.126:3128', '183.77.250.45:3128', '218.17.252.34:3128', '58.9.99.41:3128', '124.207.132.242:3128', '60.160.34.4:3128', '61.153.145.202:25', '175.154.229.72:8998', '112.91.208.78:9999', '222.33.192.238:8118', '116.242.227.201:3128', '119.29.232.113:3128', '61.136.115.147:3128', '121.248.112.20:3128', '60.21.132.218:63000', '123.7.115.141:9999', '221.212.221.194:3128', '124.88.67.32:81', '111.1.3.36:8000', '120.194.18.90:81', '114.215.150.13:3128', '120.52.21.132:8082', '218.67.126.15:3128']\n# html = request.urlopen(\"http://haoip.cc/tiqu.htm\").read().decode('utf-8')\n# iplistn= re.findall(r'r/>(.*?)<b', html, re.S) ##表示从html.text中获取所有r/><b中的内容，re.S的意思是包括匹配包括换行符，findall返回的是个list哦！\n# for ip in iplistn:\n#     i = re.sub('\\n', '', ip)##re.sub 是re模块替换的方法，这儿表示将\\n替换为空\n#     iplist.append(i.strip()) ##添加到我们上面初始化的list里面, i.strip()的意思是去掉字符串的空格\n# print(iplist)\n\n#发送请求\ndef get(url, timeout =5, proxy=None, num_retries=10):\n    UA = random.choice(user_agent_list)  ##从user_agent_list中随机取出一个字符串\n    headers = {'User-Agent':UA}\n    if proxy == None:\n        try:\n            return requests.get(url, headers=headers, timeout=timeout)  ##这样服务器就会以为我们是真的浏览器了\n        except:  ##如过上面的代码执行报错则执行下面的代码\n            if num_retries > 0:  ##num_retries是我们限定的重试次数\n                time.sleep(3)  ##延迟3秒\n                print(u'获取网页出错，3S后将获取倒数第：', num_retries, u'次')\n                return get(url, timeout,None,num_retries - 1)  ##调用自身 并将次数减1\n            else:\n                print(u'开始使用代理')\n                time.sleep(3)\n                IP = ''.join(str(random.choice(iplist)).strip())  ##下面有解释哦\n                proxy = {'http': IP}\n                return get(url,proxy=proxy)  ##代理不为空的时候\n    else:\n        try:\n            IP = ''.join(str(random.choice(iplist)).strip())  ##将从self.iplist中获取的字符串处理成我们需要的格式（处理了些什么自己看哦，这是基础呢）\n            proxy = {'http': IP}  ##构造成一个代理\n            return requests.get(url, headers=headers, proxies=proxy, timeout=timeout)  ##使用代理获取response\n        except:\n            if num_retries > 0:\n                time.sleep(3)\n                IP = ''.join(str(random.choice(iplist)).strip())\n                proxy = {'http': IP}\n                print(u'正在更换代理，3S后将重新获取倒数第', num_retries, u'次')\n                print(u'当前代理是：', proxy)\n                return get(url, timeout, proxy, num_retries - 1)\n            else:\n                print(u'代理也不好使了！取消代理')\n                return get(url)\n##保存图片,默认保存在python程序所在目录\ndef save(name, img):\n   f = open(name + '.jpg', 'ab')\n   f.write(img.content)\n   f.close()\n\n\ni = 5510 #观察得出\nwhile i >= 1:\n    #爬虫入口\n    url_src =  \"http://www.meizitu.com/a/\" + str(i) + \".html\"\n    html = get(url_src)\n    #规避错误，绕行\n    error_soup = BeautifulSoup(html.text,'lxml')\n    error = error_soup.find('a',id='eCode')\n    print(error)\n    if(error != None):\n        i -= 1\n        continue\n    target = html.text.encode('latin1').decode('gb2312', 'ignore')\n    soup = BeautifulSoup(target, 'lxml')\n    all_pic = soup.find('div',id='picture').find_all('img')\n    for pic in all_pic:\n        pic_src = pic.attrs['src']\n        pic_alt = pic.attrs['alt']\n        print(pic_src + \"    \" + pic_alt)#无聊的时候打印点东西\n        # 这里的指定存储路径，需要注意的是这里需手动创建文件夹，如需自动想、可以使用os库\n        #request.urlretrieve(pic_src, 'D:\\\\meizi\\\\' + '%s.jpg' % pic_alt)  # 指定目录位置\n        res = get(pic_src)\n        save(pic_alt,res)\n\n\n    print(i)\n    i-=1\n\nprint('over')#彻底停止", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/xiaoping0915/article/details/64918545", "title": "【Python】健壮的爬虫"}
{"timestamp": "2018_08_13_17_28_54", "desc": "Scrapy框架\n\n(本文只做学习使用,请勿他用)\n\n\n\n1.需求工具 pycharm  小说网的域名 (www.qisuu.com)\n\n第一步—–创建文件\n\n\n\n创建成功后显示如图:\n\n\n\n\n\n第二步——将创建在桌面上的scrapy文件用pycharm打开:\n\n\n\n这是创建成功后在pycharm中的显示\n\n\n\npycharm左下角打开 Terminal\n\n\n\n打开后如图 我第一次键入了一条命令  提示爬虫名字不能和项目名称一样,更改后再运行..成功建立爬虫文件 booksspider\n\n\n\n创建爬虫文件命令:    scrapy+ genspider+ 蜘蛛名称 +网站域名\n\n\n\n\n\n创建成功后,出现爬虫文件:\n\n\n\n\n\n接下来,就可以在爬虫文件中写爬虫代码了\n\n第三步——–编写爬虫代码\n\n\n\n1.红框框起来的头部 有一个是自带的,我提前 引用了几个 接下来我需要用到的 功能模块在这里就不再详细解释模块功能,下文用到后再解释.\n\n2.椭圆里面的内容 填写你爬取开始的页面URL,这里是自动生成的,一般是不正确的,需要自己打开要爬取的初始页,将URL复制到这里.\n\n\n\n\n\n3.代码思路\n\n1)请求导航条上的每个按钮对应的页面 \n 2)分别解析每个页面的电子书列表(主要获得电子书的详情url) \n 3)请求详情url,解析电子书的详细信息(书名,封面,评分,大小…下载地址) \n 4)根据下载地址下载电子书到本地\n\n获取导航栏文字及链接\n\n    def parse(self, response):\n        a_list = response.xpath(\"//div[@class='nav']/a[@target='_blank']\")\n        for a in a_list:\n            # 分类名称\n            category_name = a.xpath(\"text()\").extract_first(\"\")\n            # 拼接完整的分类url\n            category_url = urlparse.urljoin(response.url, a.xpath(\"@href\").extract_first(\"\"))\n            # 将分类地址转发给downloader下载并将结果传给parse_books_list\n            # meta:专门用来传递参数,类型是字典\n            yield scrapy.Request(\n                url=category_url,\n                callback=self.parse_books_list,\n                meta={\"category_name\": category_name, }\n            )\n\n获取每本书链接\n\ndef parse_books_list(self, response):\n        href_list = response.xpath(\"//div[@class='listBox']/ul/li/a/@href\").extract()\n        for href in href_list:\n            list_href = urlparse.urljoin(response.url, href)\n            yield scrapy.Request(\n                url=list_href,\n                callback=self.parse_books_detail,\n                meta=response.meta,\n                # meta={\"category_name\": response.meta['category_name'],}\n            )\n        all_pages = response.xpath(\"//select[@name='select']/option/@value\").extract()\n        for page in all_pages:\n            detail_url = urlparse.urljoin(response.url, page)\n            yield scrapy.Request(\n                url=detail_url,\n                callback=self.parse_books_list,\n                meta=response.meta\n            )\n\n进入书本详细页 获取书本详细信息及 下载链接 封面链接\n\ndef parse_books_detail(self, response):\n        info_div = response.xpath(\"//div[@class='detail_right']\")\n        title = info_div.xpath(\"h1/text()\").extract_first(\"\")\n        li_list = info_div.xpath(\"ul/li\")\n        size = li_list[2].xpath(\"text()\").extract_first(\"\")\n        size = size.replace(u\"文件大小：\", \"\").strip()\n        date_time = li_list[4].xpath(\"text()\").extract_first(\"\")\n        date_time = date_time.replace(u\"发布日期：\", \"\").strip()\n        user = li_list[6].xpath(\"a/text()\").extract_first(\"\")\n        download_times = li_list[1].xpath(\"text()\").extract_first(\"\")\n        download_times = download_times.replace(u\"下载次数：\", \"\").strip()\n        book_degree = li_list[7].xpath(\"em/@class\").extract_first(\"\")\n        book_degree = book_degree.replace(\"lstar\", \"\").strip()\n        download_url = response.xpath(\"//a[@class='downButton']/@href\")[1].extract()\n        img_url = response.xpath(\"//div[@class='detail_pic']/img/@src\").extract_first(\"\")\n        img_url = urlparse.urljoin(response.url, img_url)\n        category_name = response.meta['category_name']\n        print title, user, date_time, category_name\n\n        item = BooksItem()\n        item['title'] = title\n        item['size'] = size\n        item['date_time'] = date_time\n        item['user'] = user\n        item['download_times'] = download_times\n        item['book_degree'] = book_degree\n        # 小说要以GBK格式进行存储\n        ########################\n        item['download_url'] = [u\"%s\" % download_url]\n        item['img_url'] = [img_url]\n        ########################注意以列表方式存储\n        item['category_name'] = category_name\n        yield item\n\n\n\n\n第四步——设置item  /与存储有关\n\n\n\n将需要存储的信息写入,如图所示:\n\n\n\n\n\n第五步——配置settings /与下载有关\n\n打开settings,找到红方框中代码,原本是被注释掉的,将 ITEM_PIPELINES{} 解注释.原本就有的内容注释掉,另外添加两条与下载图片与文本的代码. 最后,在ITEM_PIPELINES{}下面键入四行代码,分别为图片和文本的下载链接与存储路径\n\n\n\n\n\n第六步——在Terminal中输入运行命令\n\n\n\nscrapy+crawl+爬虫名称\n\n\n\n运行后就会出现这个啦,图片和文字全部存入这两个文件夹中\n\n\n\n\n\n以上内容为 比较粗糙,因为本人也不太熟,仅做参考.懒癌犯了~ 以后完善", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/han_yanlong/article/details/77073378", "title": "Scrapy框架爬取详细步骤"}
{"timestamp": "2018_08_13_17_28_54", "desc": "写在前头\n\n在我们了解了 Netty 之后我们知道了 Netty 是一个网络框架，支持众多网络协议，其中就包括 WebSocket 协议。今天我们就使用 Netty 的这部分功能结合 SpringBoot 来构建一个实时通讯的应用。这里贴上一张图来看一下我们要到达的效果。 \n\n\n我们在写这个应用需要弄明白 Netty 的基本概念，SpringBoot 的基本使用以及 WebSocket 的基础知识。 \n- SpringBoot 开箱使用 (一) \n- Java 网络框架 Netty（一）—— 概念 \n- WebSocket 学习笔记（一）\n\n如果没有掌握相关的基础知识，建议可以先去学习一下自己不具备的相关知识。\n\n\n\n整理思路\n\n咱们需要写一个聊天室， 主要需要使用到的是 Netty 的 WebSocket 能力，就想上一篇中我们使用到了 Netty 的HTTP 能力一样，我们这里使用 SpringBoot 只是为了给浏览器返回静态的 Html 页面，当然我们也可以不需要 Spring 这一部分。直接用浏览器打开本地的 Html 文件查看。\n\n\n\nCode\n\n我们这里只看 Netty 构建 WebSocket 服务这一块的核心代码。\n\n\nNettyConfig.java\n\n\n\n\npublic class NettyConfig {\n\n    // 存储所有连接的 channel\n    public static ChannelGroup group = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE);\n    // host name 和监听的端口号，需要配置到配置文件中\n    public static String WS_HOST = \"127.0.0.1\";\n    public static int WS_PORT = \"9090\";\n\n}\n\n\nWebSocketHandler.java \n和上一篇中构建 Http 的 Handler 一样，我们这里需要一个 WebSocketHandler 来处理进来的连接。我们让 WebSocketHandler 继承自 SimpleChannelInboundHandler\n\n\n\n\npublic class WebSocketHandler extends SimpleChannelInboundHandler<Object> {\n\n    private WebSocketServerHandshaker handshaker;\n\n    private static final Logger logger = LoggerFactory.getLogger(WebSocketHandler.class);\n\n    private Gson gson = new Gson();\n\n    // onmsg\n    // 有信号进来时\n    @Override\n    protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception {\n        if(msg instanceof FullHttpRequest){\n            handHttpRequest(ctx, (FullHttpRequest) msg);\n        }else if(msg instanceof WebSocketFrame){\n            handWsMessage(ctx, (WebSocketFrame) msg);\n        }\n    }\n\n    // onopen\n    // Invoked when a Channel is active; the Channel is connected/bound and ready.\n    // 当连接打开时，这里表示有数据将要进站。\n    @Override\n    public void channelActive(ChannelHandlerContext ctx) throws Exception {\n        NettyConfig.group.add(ctx.channel());\n    }\n\n    // onclose\n    // Invoked when a Channel leaves active state and is no longer connected to its remote peer.\n    // 当连接要关闭时\n    @Override\n    public void channelInactive(ChannelHandlerContext ctx) throws Exception {\n        broadcastWsMsg( ctx, new WsMessage(-11000, ctx.channel().id().toString() ) );\n        NettyConfig.group.remove(ctx.channel());\n    }\n\n    // onmsgover\n    // Invoked when a read operation on the Channel has completed.\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {\n        ctx.flush();\n    }\n\n    // onerror\n    // 发生异常时\n    @Override\n    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {\n        cause.printStackTrace();\n        ctx.close();\n    }\n\n    // 集中处理 ws 中的消息\n    private void handWsMessage(ChannelHandlerContext ctx, WebSocketFrame msg) {\n        if(msg instanceof CloseWebSocketFrame){\n            // 关闭指令\n            handshaker.close(ctx.channel(), (CloseWebSocketFrame) msg.retain());\n        }\n\n        if(msg instanceof PingWebSocketFrame) {\n            // ping 消息\n            ctx.channel().write(new PongWebSocketFrame(msg.content().retain()));\n        }else if(msg instanceof TextWebSocketFrame){\n            TextWebSocketFrame message = (TextWebSocketFrame) msg;\n            // 文本消息\n            WsMessage wsMessage = gson.fromJson(message.text(), WsMessage.class);\n            logger.info(\"接收到消息：\" + wsMessage);\n            switch (wsMessage.getT()){\n                case 1: // 进入房间\n                    // 给进入的房间的用户响应一个欢迎消息，向其他用户广播一个有人进来的消息\n                    broadcastWsMsg( ctx, new WsMessage(-10001,wsMessage.getN()) );\n                    AttributeKey<String> name = AttributeKey.newInstance(wsMessage.getN());\n                    ctx.channel().attr(name);\n                    ctx.channel().writeAndFlush( new TextWebSocketFrame( \n                        gson.toJson(new WsMessage(-1, wsMessage.getN()))));\n                    break;\n\n                case 2: // 发送消息\n                    // 广播消息\n                    broadcastWsMsg( ctx, new WsMessage(-2, wsMessage.getN(), wsMessage.getBody()) );\n                    break;\n                case 3: // 离开房间.\n                    broadcastWsMsg( ctx, new WsMessage(-11000, wsMessage.getN(), wsMessage.getBody()) );\n                    break;\n            }\n        }else {\n            // donothing, 暂时不处理二进制消息\n        }\n    }\n\n    // 处理 http 请求，WebSocket 初始握手 (opening handshake ) 都始于一个 HTTP 请求\n    private void handHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) {\n        if(!req.decoderResult().isSuccess() || !(\"websocket\".equals(req.headers().get(\"Upgrade\")))){\n            sendHttpResponse(ctx, new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, \n                HttpResponseStatus.BAD_REQUEST));\n            return;\n        }\n        WebSocketServerHandshakerFactory factory = new WebSocketServerHandshakerFactory(\"ws://\" \n            + NettyConfig.WS_HOST + NettyConfig.WS_PORT, null, false);\n        handshaker = factory.newHandshaker(req);\n        if(handshaker == null){\n            WebSocketServerHandshakerFactory.sendUnsupportedVersionResponse(ctx.channel());\n        } else {\n            handshaker.handshake(ctx.channel(), req);\n        }\n    }\n\n    // 响应非 WebSocket 初始握手请求\n    private void sendHttpResponse(ChannelHandlerContext ctx,  DefaultFullHttpResponse res) {\n        if(res.status().code() != 200){\n            ByteBuf buf = Unpooled.copiedBuffer(res.status().toString(), CharsetUtil.UTF_8);\n            res.content().writeBytes(buf);\n            buf.release();\n        }\n        ChannelFuture f = ctx.channel().writeAndFlush(res);\n        if(res.status().code() != 200){\n            f.addListener(ChannelFutureListener.CLOSE);\n        }\n    }\n\n    // 广播 websocket 消息（不给自己发）\n    private void broadcastWsMsg(ChannelHandlerContext ctx, WsMessage msg) {\n        NettyConfig.group.stream()\n                .filter(channel -> channel.id() != ctx.channel().id())\n                .forEach(channel -> {\n                    channel.writeAndFlush( new TextWebSocketFrame( gson.toJson( msg )));\n                });\n    }\n}\n\n\nServerInitializer.java \n使用 ChannelPipeline 将各种 ChannerHandler 串起来\n\n\n\n\npublic class ServerInitializer extends ChannelInitializer<Channel> {\n\n    @Override\n    protected void initChannel(Channel channel) throws Exception {\n        ChannelPipeline pipeline = channel.pipeline();\n        pipeline.addLast(\"http-codec\", new HttpServerCodec());\n        pipeline.addLast(\"aggregator\",new HttpObjectAggregator(65536));\n        pipeline.addLast(\"http-chunked\", new ChunkedWriteHandler());\n        pipeline.addLast(\"handler\", new WebSocketHandler());\n    }\n}\n\n\nServerBootStrap.java \n启动和销毁 Netty 的程序\n\n\n\n\n@Component\npublic class ServerBootStrap {\n    private final EventLoopGroup bossGroup = new NioEventLoopGroup();\n    private final EventLoopGroup workGroup = new NioEventLoopGroup();\n    private Channel channel;\n\n    public ChannelFuture start(InetSocketAddress address) {\n        ServerBootstrap bootstrap = new ServerBootstrap();\n        bootstrap.group(bossGroup, workGroup)\n                .channel(NioServerSocketChannel.class)\n                .childHandler(new ServerInitializer())\n                .option(ChannelOption.SO_BACKLOG, 128)\n                .childOption(ChannelOption.SO_KEEPALIVE, true);\n\n        ChannelFuture future = bootstrap.bind(address).syncUninterruptibly();\n        channel = future.channel();\n        return future;\n    }\n\n    public void destroy() {\n        if(channel != null) {\n            channel.close();\n        }\n        NettyConfig.group.close();\n        workGroup.shutdownGracefully();\n        bossGroup.shutdownGracefully();\n    }\n}\n\n\n  由于我们需要使用到 SpringBoot，当然你也可以不用，你可以写个 main 方法，然后在里面 new 一个 ServerBootStrap 的实例调用其 start 方法即可，我们这里让 SpringBoot 的主程序 App.java 实现 CommandLineRunner 接口，然后在 run 方法中这样操作。\n\n\n\n\n@SpringBootApplication\npublic class App implements CommandLineRunner{\n\n    private static final Logger logger = LoggerFactory.getLogger(App.class);\n\n    @Autowired\n    private ServerBootStrap ws;\n\n    public static void main(String[] args) throws Exception {\n        // SpringApplication 将引导我们的应用，启动 Spring，相应地启动被自动配置的 Tomcat web 服务器。    \n        SpringApplication.run(App.class, args);\n    }\n\n    // 注意这里的 run 方法是重载自 CommandLineRunner\n    @Override\n    public void run(String... args) throws Exception {\n        logger.info(\"Netty's ws server is listen: \" + NettyConfig.WS_PORT);\n        InetSocketAddress address = new InetSocketAddress(NettyConfig.WS_HOST, NettyConfig.WS_PORT);\n        ChannelFuture future = ws.start(address);\n\n        Runtime.getRuntime().addShutdownHook(new Thread(){\n            @Override\n            public void run() {\n                ws.destroy();\n            }\n        });\n\n        future.channel().closeFuture().syncUninterruptibly();\n    }\n\n}\n\n前端的代码，就不贴在这儿了，前端的代码逻辑很简单，在代码仓库中对应 \n- 页面  /resources/templates/chat.html \n- js  /resources/public/js/chat.js \n- css  /resources/public/css/chat.css\n\n代码仓库\n\nhttps://github.com/xiaop1ng/PlayWithSpringBoot", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/xiaoping0915/article/details/81202851", "title": "Java 网络框架 Netty（三）—— 和 SpringBoot 一起打造聊天室应用"}
{"timestamp": "2018_08_13_17_28_54", "desc": "有话想说\n\n之前的博文，每每写到一个新的东西都会在最前头写上 xx 是啥？今天想换一种方式，今天先不说 Dubbo 是啥？因为这玩意一时半会儿写不明白，也有可能是博主愚昧，看了半天官方文档也没能理解这到底是干嘛的一个玩意儿阿。直到跟着官方给出的 Demo 程序写出第一个 Dubbo 应用（没错，是 Hello World 程序），后来又有幸和使用过 Dubbo 的同事讨论了一下  Dubbo 才云消雾散。 \n所以这次我们先不去看那些概念了，先实现一个应用（不要小瞧 Hello World）\n\n思考\n\n首先一起思考一个问题，如何把应用从单机扩展到分布式？不妨先自己思考几分钟后再去利用搜索引擎。\n\n\n\nHelloWorld 程序\n\n不知道大家对于生产消费模型熟不熟悉，消息队列（RabbitMQ、Apache ActiveMQ 等）的架构就是这样的。接着我们要实现的这个 Demo 也可以类比这种模型去理解，我们写一个 Server 作为服务提供者（Provider），写一个Client 作为服务消费者（Consumer）通过 Dubbo 来调用 Server 上提供的一个服务。\n\n\n\n0. 使用 Maven 构建 Provider\n\n\npom.xml \n采用 Spring 加载 Dubbo 的配置的方式，所以依赖了 Spring 和 Dubbo，需要注意的是在构建消费者程序时同样依赖 Spring 和 Dubbo\n\n\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dubbo</artifactId>\n    <version>2.6.2</version>\n</dependency>\n\n<dependency>\n    <groupId>org.springframework</groupId>\n    <artifactId>spring-context</artifactId>\n    <version>5.0.7.RELEASE</version>\n</dependency>\n\n\n\n1. 定义服务提供者的服务接口\n\n\ncom.xiaoping.dubbotest.HelloService.java\n\n\n\n\npublic interface HelloService {\n    String sayHello(String name);\n}\n\n2. 服务接口的实现类\n\n\ncom.xiaoping.dubbotest.HelloServiceImpl.java\n\n\npublic class HelloServiceImpl implements HelloService {\n    Logger logger = LoggerFactory.getLogger(HelloServiceImpl.class);\n    public String sayHello(String name) {\n        logger.info(\"From Provider\" + name);\n        return \"Hello \" + name;\n    }\n}\n\n3. 服务提供者的配置文件\n\n\napplication.xml \n这里的配置文件是直接放在 resources 根目录下的。\n\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:dubbo=\"http://code.alibabatech.com/schema/dubbo\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\">\n    <!-- 提供方应用信息，用于计算依赖关系 -->\n    <dubbo:application name=\"demo-provider\"/>\n    <!-- 使用multicast广播注册中心暴露服务地址 -->\n    <dubbo:registry address=\"multicast://224.5.6.7:1234\"/>\n    <!-- 用dubbo协议在20880端口暴露服务 -->\n    <dubbo:protocol name=\"dubbo\" port=\"20880\"/>\n    <!-- 声明需要暴露的服务接口 -->\n    <dubbo:service interface=\"com.xiaoping.dubbotest.HelloService\" ref=\"helloService\"/>\n    <!-- 和本地bean一样实现服务 -->\n    <bean id=\"helloService\" class=\"com.xiaoping.dubbotest.HelloServiceImpl\"/>\n</beans>\n\n4. 服务提供者的入口程序\n\n\nProviderApp.java\n\n\n\n\npublic class ProviderApp {\n    public static void main(String[] args) throws IOException {\n        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\n                new String[] {\"application.xml\"});\n        context.start();\n        System.in.read(); // press any key to exit\n    }\n}\n\n5. 消费者程序的配置文件\n\n\napplication.xml\n\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:dubbo=\"http://code.alibabatech.com/schema/dubbo\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\">\n    <!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 -->\n    <dubbo:application name=\"demo-consumer\"/>\n    <!-- 使用multicast广播注册中心暴露发现服务地址 -->\n    <dubbo:registry address=\"multicast://224.5.6.7:1234\"/>\n    <!-- 生成远程服务代理，可以和本地bean一样使用demoService -->\n    <dubbo:reference id=\"helloService\" interface=\"com.xiaoping.dubbotest.HelloService\"/>\n</beans>\n\n6. 消费者的入口程序\n\n\nConsumer.java\n\n\n\n\npublic class Consumer {\n    public static void main(String[] args) {\n        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\n                new String[]{\"application.xml\"});\n        context.start();\n        // obtain proxy object for remote invocation\n        HelloService helloService = (HelloService) context.getBean(\"helloService\"); \n        // 注： 这里的 HelloService 可以是依赖自 Provider 中的接口，也可以是一个和他一样的副本\n        String hello = helloService.sayHello(\"world\"); // execute remote invocation\n        System.out.println(hello); // show the result\n    }\n}\n\n7. 先启动 ProviderApp,再启动 Consumer\n\nConsumer 输出：\n\n7月 25, 2018 9:51:39 下午 org.springframework.context.support.AbstractApplicationContext prepareRefresh\n信息: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@13a5fe33: startup date [Wed Jul 25 21:51:39 CST 2018]; root of context hierarchy\n7月 25, 2018 9:51:39 下午 org.springframework.beans.factory.xml.XmlBeanDefinitionReader loadBeanDefinitions\n信息: Loading XML bean definitions from class path resource [application.xml]\n7月 25, 2018 9:51:39 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息: using logger: com.alibaba.dubbo.common.logger.jcl.JclLoggerAdapter\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by javassist.ClassPool (file:/Users/xiaop1ng/apache-maven-3.5.3/repo/org/javassist/javassist/3.20.0-GA/javassist-3.20.0-GA.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\nWARNING: Please consider reporting this to the maintainers of javassist.ClassPool\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Register: consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=consumers&check=false&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Send broadcast message: register consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=consumers&check=false&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 to /224.5.6.7:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: register consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=consumers&check=false&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Subscribe: consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Send broadcast message: subscribe consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 to /224.5.6.7:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: subscribe consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: register dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Notify urls for subscribe url consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351, urls: [dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590], dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Successed connect to server /192.168.2.184:20880 from NettyClient 192.168.2.184 using dubbo version 2.6.2, channel is NettyChannel [channel=[id: 0x6911f93f, /192.168.2.184:50565 => /192.168.2.184:20880]], dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Start NettyClient bogon/192.168.2.184 connect to the server /192.168.2.184:20880, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Notify urls for subscribe url consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351, urls: [dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590], dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Refer dubbo service com.xiaoping.dubbotest.HelloService from url multicast://224.5.6.7:1234/com.alibaba.dubbo.registry.RegistryService?anyhost=true&application=demo-consumer&check=false&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&register.ip=192.168.2.184&remote.timestamp=1532526254590&side=consumer&timestamp=1532526700351, dubbo version: 2.6.2, current host: 192.168.2.184\nHello world\n\nProviderApp 输出：\n\n\n\n7月 25, 2018 9:44:13 下午 org.springframework.context.support.AbstractApplicationContext prepareRefresh\n信息: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@13a5fe33: startup date [Wed Jul 25 21:44:13 CST 2018]; root of context hierarchy\n7月 25, 2018 9:44:13 下午 org.springframework.beans.factory.xml.XmlBeanDefinitionReader loadBeanDefinitions\n信息: Loading XML bean definitions from class path resource [application.xml]\n7月 25, 2018 9:44:13 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息: using logger: com.alibaba.dubbo.common.logger.jcl.JclLoggerAdapter\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by javassist.ClassPool (file:/Users/xiaop1ng/apache-maven-3.5.3/repo/org/javassist/javassist/3.20.0-GA/javassist-3.20.0-GA.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\nWARNING: Please consider reporting this to the maintainers of javassist.ClassPool\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n7月 25, 2018 9:44:14 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] The service ready on spring started. service: com.xiaoping.dubbotest.HelloService, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:14 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Export dubbo service com.xiaoping.dubbotest.HelloService to local registry, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:14 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Export dubbo service com.xiaoping.dubbotest.HelloService to url dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&bind.ip=192.168.2.184&bind.port=20880&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:14 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Register dubbo service com.xiaoping.dubbotest.HelloService url dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&bind.ip=192.168.2.184&bind.port=20880&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 to registry registry://224.5.6.7:1234/com.alibaba.dubbo.registry.RegistryService?application=demo-provider&dubbo=2.6.2&pid=1027&registry=multicast&timestamp=1532526254536, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Start NettyServer bind /0.0.0.0:20880, export /192.168.2.184:20880, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Register: dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Send broadcast message: register dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 to /224.5.6.7:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: register dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Subscribe: provider://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&category=configurators&check=false&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Send broadcast message: subscribe provider://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&category=configurators&check=false&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 to /224.5.6.7:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:15 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: subscribe provider://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&category=configurators&check=false&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:44:16 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger warn\n警告:  [DUBBO] Ignore empty notify urls for subscribe url provider://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&category=configurators&check=false&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: register consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=consumers&check=false&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: subscribe consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Send broadcast message: register dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 to /224.5.6.7:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: register dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&dubbo=2.6.2&generic=false&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] From Providerworld, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: unregister consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=consumers&check=false&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: unregister consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] Receive multicast message: unsubscribe consumer://192.168.2.184/com.xiaoping.dubbotest.HelloService?application=demo-consumer&category=providers,configurators,routers&dubbo=2.6.2&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1039&side=consumer&timestamp=1532526700351 from /192.168.2.184:1234, dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger warn\n警告:  [DUBBO] All clients has discontected from /192.168.2.184:20880. You can graceful shutdown now., dubbo version: 2.6.2, current host: 192.168.2.184\n7月 25, 2018 9:51:40 下午 com.alibaba.dubbo.common.logger.jcl.JclLogger info\n信息:  [DUBBO] disconnected from /192.168.2.184:50565,url:dubbo://192.168.2.184:20880/com.xiaoping.dubbotest.HelloService?anyhost=true&application=demo-provider&bind.ip=192.168.2.184&bind.port=20880&channel.readonly.sent=true&codec=dubbo&dubbo=2.6.2&generic=false&heartbeat=60000&interface=com.xiaoping.dubbotest.HelloService&methods=sayHello&pid=1027&side=provider&timestamp=1532526254590, dubbo version: 2.6.2, current host: 192.168.2.184\n\n\n\n写在后头\n\n在跑通了 Demo 程序之后我们来看一下 Dubbo 的架构\n\n\n\nProvider 暴露服务的服务提供方 \nConsumer 调用远程服务的服务消费方 \nRegistry 服务注册与发现的注册中心 \nMonitor 统计服务的调用次数和调用时间的监控中心 \nContainer 服务运行容器\n\n现在来看这张架构图应该清晰一些，Provider 和 Consumer 使用接口作为服务的契约，通过注册中心 Registry 来完成服务的注册和发现，远程通讯的细节通过代理类来屏蔽。\n\n了解更多\n\n\n官方文档\nDubbo 用户手册\nDubbo 开发手册\nDubbo 管理手册", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/xiaoping0915/article/details/81211931", "title": "分布式服务框架 Dubbo 入门（一）"}
{"timestamp": "2018_08_13_17_28_54", "desc": "当前环境 系统 Ubuntu 14.04LTS  使用apt-get的方式来安装需要的软件快速搭建java环境 \n0.搜索相关安装包，为了更快的下载（Oracle官网下载会稍微慢一些）\n\n\n\nsudo apt-cache search jdk\n\n1.安装jdk7，期间会询问是否同意（Y/n）选择同意 Y 即可\n\nsudo apt-get install openjdk-7-jdk\n\n2.检查java环境\n\n\n\njava -version\njavac -version\n\n3.安装Tomcat7\n\n\n\nsudo apt-get install tomcat6 \n\n4.启动Tomcat7\n\n\n\n/etc/init.d/tomcat7 start\n\n5.访问   服务器地址：8080 \n看到 It works ! 字样的网页 就说明环境已经搭建完成", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/xiaoping0915/article/details/59078208", "title": "【服务器】在Linux下搭建java环境"}
{"timestamp": "2018_08_13_17_28_54", "desc": "今天还是带来的是一个用scrapy抓取图片的实战项目\n\n这次用scrapy的图片管道进行，下一篇我会讲解用自制管道进行图片的下载哦！\n\n具体的步骤你们可以去官网看看\n\n我这里主要会将一些步骤和常见的坑。。。\n\n这次我们先将settings.py文件，因为这次的坑主要会在这个文件中，对于抓取链接什么的，在前两篇文章已经将的很清楚了。\n\n由于是要抓取图片，所以这次在settings中的相关设置会有些不一样。和往常一样，我们要打开这些设置：\n\n\n\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'\nDOWNLOAD_DELAY = 3\n# 下载延迟和请求用户的掩饰\n\n接下来有一些关键的设置\n\n\n\nITEM_PIPELINES = {\n   'scrapy.pipelines.images.ImagesPipeline':300\n}\n# 使用scrapy的下载图片的专用管道（这里使用默认，下一讲用自己定义的管道）\nIMAGES_STORE = 'D:/scrapy项目/pictureproject/pictures'\n# 图片下载的地址\nIMAGES_URLS_FIELD = 'our_image_urls'\n# 爬取的图片的链接\nIMAGES_RESULT_FIELD = 'our_images'\n# 存储图片信息的\n\n\n注意： \n    一、第一个TIEM_PIPELINES中我们传入的是下载图片的管道，与之前的不一样（默认会给我们传入项目建立时生成的管道，需要修改）\n\n二、还有就是，IMAGES_URLS_FIELD和IMAGES_RESULTE_FIELD的值使我们在items中定义的，如果我们不再items中定义，images_urls和images就是默认的值（请下面查看items.py文件）\n\n\n\nROBOTSTXT_OBEY = False\n# 是否遵守机器人协议，默认值是True，改为False\n\n每个网站都会有一个robot.txt文件，里面规定了机器人爬取的一些规定，但对于爬取的图片的网站来说，一般对定是不允许进行爬取图片的，所以不能遵守（额。。。其实官方是建议遵守 的）\n\n好了，以上是settings.py文件的设置\n\n接下来是items.py文件 \n代码如下：\n\n\n\nimport scrapy\n\n\nclass PictureprojectItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    our_image_urls = scrapy.Field()\n    our_images = scrapy.Field()\n# 这里我们自己定义了图片链接和信息存放的item\n\n如上面说的，我们如果不再这里定义item的话，就会用image_urls和images做为存放的item \n那样的话，代码就很简单了 \n首先setting.py里面：\n\n\n\nIMAGES_URLS_FIELD = 'image_urls'\nIMAGES_RESULT_FIELD = 'images'\n# 这两个是不需要的\n\n接下来items.py里面：\n\n\n\nimport scrapy\n\n\nclass PictureprojectItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n     pass\n\n这样的话就会按默认的来。。。。。。。\n\n最后就是一个爬虫的文件了，这个文件你只要爬取了图片链接信息并存储到item里面，不过这里面也会有一个坑，注意一下 \n代码如下：\n\n\n\nimport scrapy\nfrom pictureproject.items import PictureprojectItem\n\n\nclass PicturespiderSpider(scrapy.Spider):\n    name = 'picturespider'\n    allowed_domains = ['pixabay.com']\n    start_urls = ['https://pixabay.com/en/users/Free-Photos-242387/']\n\n    def parse(self, response):\n        picture_url = response.xpath('.//div[@class=\"flex_grid credits\"]/div[@class=\"item\"]')\n        for picture in picture_url:\n            pic = picture.xpath('./a/img/@src').extract()\n            # 坑在这里，图片的链接必须是含一个元素的数组\n            item = PictureprojectItem(our_image_urls=pic)\n            # 如果选择默认值，将our_image_urls改为image_urls就可以了\n            yield item\n\n这样就可以爬取图片了，当然，网站自选，爬虫的编写自选。\n\n对了，最后将一下我们的pipelines.py文件 \n额。。。你不用动它。\n\n输入：scrapy crawl picturespider \n结果： \n\n\n下一篇我们会讲到怎么自制图片下载的管道，那时候就要动用pipilines.py文件了，嗯！还有怎么设置图片的大小什么的。\n\n额。。。。。。。。。。。。。 \n有什么问题欢迎和我一起交流哦！", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/killeri/article/details/80222447", "title": "scrapy初探（抓取图片）"}
{"timestamp": "2018_08_13_17_28_54", "desc": "1.安装H2o\n\ncmd:输入pip install h2o\n\n\n\n\n\n2.启动\n\n\n\npython\nimport h2o\nh2o.init()\n\n\n\n启动成功之后可以在浏览器中输入：localhost:54321 \n\n\n\n\n3.数据准备\n\n使用到的数据在我的github \n\n\n\n\n3.建模\n\na.读入数据 \n\n\n\n\nb.解析文件 \n\n\nc.查看Job \n\n\nd.创建模型 \n\n\ne.选择随机森林 \n \nf.选择特征，响应列，其他参数按需调整 \n \ng.参数填好之后，建模\n\n\n\nh.查看Job \n \ni.随机森林数的数量与trainlogloess之间的关系 \n \nj.各个属性的重要程度 \n\n\n\n\n4.分类\n\na.导入测试集 \n \nb.接下来几个步骤建模时候的一样，这里就不赘述 \nc.预测 \n\n\n\n\nd.分类结果 \n\n\ne.将分类结果与测试集合并 \n\n\nf.将合并结果导出 \n\n\n\n\n进度条跑到百分之百就说明导出成功 \n\n\n得到的结果，比之前的测试集多了三列\n\n\n\n\n\n5.使用IDE的代码编写\n\n\n\n\n# coding: utf-8\n\n# In[1]:\n\nimport h2o\nh2o.init()\n\n\n# In[75]:\n\ntrainFrame =h2o.import_file(\"C:\\\\Users\\\\gpwner\\\\Desktop\\\\train.csv\")[2:]\nnames=trainFrame.col_names[:-1]\nresponse_column = 'Catrgory'\n\n\n# In[37]:\n\nfrom h2o.estimators import H2ORandomForestEstimator\n# Define model\nmodel = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10)\nmodel.train(x=names,y=response_column,training_frame=trainFrame)\n\n\n# In[84]:\n\ntestdata =h2o.import_file(\"C:\\\\Users\\\\gpwner\\\\Desktop\\\\test.csv\")[2:]\npre_tag=H2ORandomForestEstimator.predict(model ,testdata)\npre_tag['predict']\nresultdata=testdata.cbind(pre_tag['predict'])\nresultdata\nh2o.download_csv(resultdata,\"C:\\\\Users\\\\gpwner\\\\Desktop\\\\predict.csv\")\n\n\n# In[82]:\n\nfrom __future__ import division\ncorrect=resultdata[resultdata['Catrgory']==resultdata['predict']]\nprint(float(len(correct)/len(resultdata)))", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/gpwner/article/details/74058850", "title": "Python<H2o使用分布式随机森林建模分类>"}
{"timestamp": "2018_08_13_17_28_54", "desc": "题目是这样的：\n\n\n  某科技公司两位科学家（甲、乙）去吃饭，坐在一家酒店靠近街道的窗口座位吃饭，在等待上菜的过程中，闲极无聊，甲向乙出了一道猜三个女儿年龄的题目。\n  \n  甲：我有3个女儿，3人年龄之积等于36；\n  \n  乙：猜不出来；\n  \n  甲：3个女儿年龄之和等于街道上的行人数；\n  \n  乙：还是无法确定；\n  \n  甲：我的大女儿叫苏珊。\n  \n  乙：哦，我知道了。\n  \n  请问，甲的3个女儿年龄各是多少？\n\n\n闲着无聊，加上这道题也有点意思，就试着用python写了一下\n\n\n\n解题思路\n\n首先我们要明白题目中给出的线索\n\n1、3个女儿的年龄积为36\n\n2、她们的年龄和已知，但是不能得到最终结果\n\n3、有一个大女儿\n\n\n\n就这3个条件，就可以得出我们想要的结果,实话说，开始我也是很懵逼，后来仔细想了想，还是有可能的，我们可以这样来做\n\n1、找出积为36的所有可能，并写入列表\n\n2、上面的列表求和，结果不唯一，则为备选答案\n\n3、列表中最大的值唯一\n\n怎么样，这样来看是不是清晰多了\n\n先来看看，怎么获取所有积为36的列表，我们新建一个函数，然后循环1-36之间的所有可能3次，最后判断3个数字相加为36即写入列表，这里注意，先将列表排序，然后在写入列表的时候就可以判断去重了。\n\n\n\n然后就是在来一个函数，主要是计算一个列表的所有元素的和\n\n\n\n最后就是主函数了，这里主要做最后的判断，先循环列表，取出所有列表的和，如果没有重复就写入一个空列表在，如果重复了，就再次进行判断，最后一个条件，最大值是否唯一，这样就可以得出我们想要的结果了！\n\n完整代码如下：\n\n\n\nemmm,学习之余写写算法，高手勿喷~！欢迎大家关注我，并一起来学习交流哦！题目是这样的：\n\n\n  某科技公司两位科学家（甲、乙）去吃饭，坐在一家酒店靠近街道的窗口座位吃饭，在等待上菜的过程中，闲极无聊，甲向乙出了一道猜三个女儿年龄的题目。\n  \n  甲：我有3个女儿，3人年龄之积等于36；\n  \n  乙：猜不出来；\n  \n  甲：3个女儿年龄之和等于街道上的行人数；\n  \n  乙：还是无法确定；\n  \n  甲：我的大女儿叫苏珊。\n  \n  乙：哦，我知道了。\n  \n  请问，甲的3个女儿年龄各是多少？\n\n\n闲着无聊，加上这道题也有点意思，就试着用python写了一下\n\n\n\n解题思路\n\n首先我们要明白题目中给出的线索\n\n1、3个女儿的年龄积为36\n\n2、她们的年龄和已知，但是不能得到最终结果\n\n3、有一个大女儿\n\n\n\n就这3个条件，就可以得出我们想要的结果,实话说，开始我也是很懵逼，后来仔细想了想，还是有可能的，我们可以这样来做\n\n1、找出积为36的所有可能，并写入列表\n\n2、上面的列表求和，结果不唯一，则为备选答案\n\n3、列表中最大的值唯一\n\n怎么样，这样来看是不是清晰多了\n\n先来看看，怎么获取所有积为36的列表，我们新建一个函数，然后循环1-36之间的所有可能3次，最后判断3个数字相加为36即写入列表，这里注意，先将列表排序，然后在写入列表的时候就可以判断去重了。\n\n\n\n然后就是在来一个函数，主要是计算一个列表的所有元素的和\n\n\n\n最后就是主函数了，这里主要做最后的判断，先循环列表，取出所有列表的和，如果没有重复就写入一个空列表在，如果重复了，就再次进行判断，最后一个条件，最大值是否唯一，这样就可以得出我们想要的结果了！\n\n完整代码如下：\n\n\n\nemmm,学习之余写写算法，高手勿喷~！欢迎大家关注我，并一起来学习交流哦！", "time": "2018_08_13_17_28_54", "link": "https://blog.csdn.net/programmer_yf/article/details/80626727", "title": "关于一道面试题的极其无聊的python算法实现"}
{"timestamp": "2018_08_13_17_28_55", "desc": "还有4天就世界杯了，作为一个资深（伪）球迷，必须要实时关注世界杯相关新闻，了解各个球队动态，这样才能在一堆球迷中如（大）鱼（吹）得（特）水（吹），迎接大家仰慕的目光!\n\n给大家分享一个快速了解相关信息的办法：刷论坛！我们来一起做个虎扑论坛的爬虫吧！\n\n\n\n抓包获取虎扑论坛相关帖子内容，逐条显示！\n\n先来观察下网页，打开论坛首页，选择国际足球\n\n\n\n然后往下拉，找到世界杯相关内容\n\n\n\n这里就是我们的目标了，所有相关的新闻都会在这里显示，用F12打开“开发者工具”然后往下浏览看看数据包\n\n\n\n注意箭头指向的那几个地方！\n\n这就是刚才浏览的新闻所在的json包，来看看具体数据是什么\n\n\n\nok，标题、地址、发布时间包括来源都已经出现了!我们可以直接抓取json数据然后取出相关内容！\n\n再进入具体新闻页面看看\n\n\n\n所有的文本内容，都在\n\n这个标签下的\n\n标签内，我们可以用xpath直接取div下的所有文本内容!\n\n这里就不一 一说明了，直接上代码，并录个小的GIF图片给大家看看效果\n\n\n\n#@author Q群542110741\n# -*- coding:utf-8 -*-\nimport requests\nfrom lxml import etree\n\nheader = {\n    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0',\n    'Host':'soccer.hupu.com',\n    'Referer':'https://soccer.hupu.com/'}\ni = 0\nwhile 1:\n    #构建循环页面翻页\n    url = 'https://soccer.hupu.com/home/latest-news?league=世界杯&page='\n    i += 1\n    #获取json数据，一页20个\n    html = requests.get(url+str(i),headers=header).json()['result']\n    for info in html:\n        time_r = info['time']#发布时间\n        title = info['title']#标题\n        url_r = info['url']#新闻链接\n        origin = info['origin']#来源\n        print(title)\n        print('发布时间:',time_r,' '*5,'来自：',origin)\n        head = header\n        head['Host'] = 'voice.hupu.com'#更改header中Host参数\n        html_r = requests.get(url_r,headers=head)#获取新闻详情\n        html_r.encoding = 'utf-8'#编码格式指定\n        #获取div下的所有文本\n        datas = etree.HTML(html_r.text).xpath('//div[@class=\"artical-content-read\"]')[0].xpath('string(.)').strip()\n        print('\\n'+'内容：'+'\\n'*2,datas,'\\n')\n        #可由用户手动退出循环\n        if input('任意键继续，“q”退出') in ['q', 'Q']:\n            exit()\n\n\n\n现在我们可以快乐的刷刷论坛，积累最新资讯，秒杀一切挡在我们前（装）进（B）道路上的渣渣吧~！\n\n\n\n欢迎大家关注，私信我一起学习，一起看球！", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/programmer_yf/article/details/80640976", "title": "世界杯快到了，看我用Python爬虫实现（伪）球迷速成！"}
{"timestamp": "2018_08_13_17_28_55", "desc": "几个非常适合新手练习python爬虫的网页，总有一款能搞定！ \n话不多说，直接干货了！\n\n头条图集:抓包获取json数据 \n打开今日头条主页，搜索小姐姐，或者其他你感兴趣的内容，然后点击图集\n\n\n\n动态加载的json数据就出来了，没有反爬，注意的是，如果不想去内容里面抓图片的话，可以只抓缩略图，就是这个页面显示的图片，它在json数据中的image_list中，注意，将url中的list换成origin，就是大图哦！代码如下\n\n \n所有的图片地址都出来了！\n\n全书网：直接源码匹配重点内容相关内容 \n直接搜索全书网，打开主页，随便找一篇小说，比如《盗墓笔记》，点击后跳转到网页，在点击开始阅读，出现了所有章节，在进入章节就出现了小说内容，网页内容和代码如下：\n\n\n\n\n\n80电子书：匹配到地址直接下载压缩文件 \n80电子书网和上面的全书网比较类似，但是它本身提供有下载功能，可以直接用小说Id和名字直接构建下载文件，页面截图和代码：\n\n\n\n \npython学习交流群542110741 \n \n其他类似网站\n\n类似的网站还有：妹子图网、美桌网、笔趣阁、久久等等甚至百度图片也可以用抓包获取数据的！\n\n以上代码都是随手写的，没有排版，大家有兴趣可以自己排下版，或者比如小说网站，可以先抓取大类别，然后在每一个类别中获取所有小说，最后在把所有小说的内容抓出来，这样就是全站爬虫了！！！\n\n\n\n如果大家还有什么适合的网站，希望能在评论区里共享下哦！大家一起交流下！", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/programmer_yf/article/details/80596041", "title": "几个非常适合新手练习python爬虫的网页，总有一款能搞定！"}
{"timestamp": "2018_08_13_17_28_55", "desc": "SpringBoot CLI\n\nCLI （Command Line Interface）， 一个可以帮助我们快速构建 SpringBoot 应用的命令行工具。\n\n我们知道 SpringBoot 已经简化 Spring 应用中很多繁琐的配置和构建工程的步骤，然而 SpringBoot CLI 工具在这个基础上继续简化了 SpringBoot 应用程序的构建。\n\nQuik Start\n\n首先需要下载 CLI，下载地址：  http://repo.spring.io/release/org/springframework/boot/spring-boot-cli/\n\n我们这里选择的是 2.0.1.RELEASE 版本 \nhttp://repo.spring.io/release/org/springframework/boot/spring-boot-cli/2.0.1.RELEASE/spring-boot-cli-2.0.1.RELEASE-bin.zip\n\n下载完成后解压，然后将解压出来的 /bin 目录添加至环境变量中 \n在 cmd 程序中输入 spring --version 会输出\n\n\n  Spring CLI v2.0.1.RELEASE\n\n\n即表明安装配置成功 \n然后我们写一段 Java 程序测试一下\n\n\nHello.java\n\n\n@RestController\npublic class Hello {\n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello, Cli\";\n    }\n}\n\n将 cmd 程序切换至 Hello.java 文件所在目录下执行 spring run Hello.java\n\n \nSpringBoot CLI 在这时会构建并启动程序，启动完成后，访问 http://127.0.0.1:8080/hello\n\n\n  Hello, Cli\n\n\n到这里我们的测试就已经通过了", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/xiaoping0915/article/details/81386788", "title": "SpringBoot CLI 的使用（三）"}
{"timestamp": "2018_08_13_17_28_55", "desc": "插入排序（insertion sorting）\n\n大体含义是这样的，想我们在打扑克牌理牌时的思路一样，来一张扑克牌做一次插入操作。\n\n\n\n下面我们给出普通版和优化版的插入排序\n\n\n\n    public int [] insertionSort(int [] arr){\n        for (int i = 1; i<arr.length;i++){\n            for (int j = i; j>0 && arr[j] < arr[j-1];j--){\n                int temp = arr[j];//循环比较两个相邻的值，满足排序条件做交换，不满足跳出当前这层循环\n                arr[j] = arr[j-1];\n                arr[j-1] = temp;\n            }\n        }\n        return arr;\n    }\n\n    public int [] insertionSortPlus(int [] arr){\n        for (int i = 1; i<arr.length;i++){\n            int x = arr[i]; //记录当前抽的数\n            int j;          //记录数的位置\n            for (j = i; j>0 && arr[j-1] >x;j--){\n                arr[j] = arr[j-1];//挪位置\n            }\n            arr[j] = x;     //最后处理当前抽的数的位置归宿 需要注意的是这里的 j 是上面 for 循环退出时的值\n        }\n        return arr;\n    }\n\n优化版的算法主要在于交换的次数上的优化，在数组本身的顺序较为良好的情况下，这种插入排序的优势可以体现出来，因为不用向冒泡或是选择排序那样必须走完内层循环，找到一个合适的时机可以提前跳出内层循环。\n\n\n\n图解算法目录\n\n【图解算法】排序算法——冒泡排序、选择排序\n\n【图解算法】排序算法——插入排序\n\n【图解算法】排序算法——归并排序\n\n【图解算法】排序算法——快速排序\n\n【图解算法】Java GC算法\n\n【图解算法】排序算法——堆排序\n\n【图解算法】并查集 —— 联合查找算法\n\nGif Power By https://visualgo.net", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/xiaoping0915/article/details/72949784", "title": "【图解算法】排序算法——插入排序"}
{"timestamp": "2018_08_13_17_28_55", "desc": "1.2 python  \n1.2.1 第三方库 \nRequests \nBeautifulSoup \n1.2.2 重要知识点\n\n\n  1.2 python  \n  1.2.1 第三方库\n\n\nRequests\n\npython开源了很多第三方库，在写爬虫抓取数据的时候，一般会用第三方库：requests，使用import，导入包之后就可以调用了。\n\nRequests:可以用来发送网络请求，如图： \n \n此外，还有很多高级的操作，如： \n传递参数：如果要手工构建URL，那么数据会以健值对的形式放在URL中，跟在一个问号后面，这在写爬虫的时候会非常方便，如图： \n \n注：字典为None的值不会传到URL里\n\n定制请求头：若想要自己的爬虫更加强壮，不被封掉，可以简单传递一个字典给headers,让自己伪装得更好。\n\n此外，还可以以字节的方式访问请求响应体，同时request还内置了json解码器，可以处理JSON数据。\n\nBeautifulSoup\n\nBeautifulSoup解析页面 \n见代码 Beautiful_basic.ipynb \n \nprettify函数可以把标签美化，变成典型的层级结构\n\nBeautifulSoup把html转换成复杂的树形结构，每个节点都是python对象，所有对象可以归纳为四种： Tag, NavigableString, BeautifulSoup, Comment\n\nTag \n只要加上标签名，就可以获取对应的标签。 \n注：这样获取的是第一个符合内容的标签 \n \n \n \n \n \n \n \n \n \n\n\n\n  1.2.2 重要知识点\n\n\n变量 \n使用变量，可以大幅提升代码的效率，也利于函数之间传递。 \n如10 + 3 = 13，2 + 10 = 12，10 – 16 = -6. \n这三个表达式都使用了10这个数值，这时候可以命名一个变量x= 10,就可以写成：x + 3 = 13, 2 + x = 12, x – 16= -6。 \n注：变量命名可以包括字母，数字，下划线，但不能以数字开头。\n\n字符串 \n字符串和数字一样，也是一个值，但字符串需要以’’或””包起来。多个字符串可以用 + 拼接起来。 \n字符串的常用函数有： \nlen(‘huang’) :输出5，获得字符串中的元素个数 \nstr.strip() :去除字符串的首尾空白符 \nstr.replace :替换字符 \nstr.split(‘i’) :根据字母i切分字符\n\n\n\n列表 \n列表是一种有序的集合，可以随时添加或删除里面的元素，列表中的每个元素都对应着一个索引号，索引从0开始。 \n\n\n字典 \n字典的重要组成部分是键（key）和值（value）,key是字典的索引，因而它一定是唯一的。 \n\n\n文件读写：必备技能 \n\n\n将文本内容写到某个文件中，path表示文件路径； \nmode表示读或者写，mode=’w’表示写，mode = ‘r’表示读； \nfp为文件对象； \nfp.write表示调用write方法，并写入字符串； \n最后关闭。", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/qq_39422642/article/details/78758359", "title": "爬虫-python基础篇"}
{"timestamp": "2018_08_13_17_28_55", "desc": "Dubbo分布式服务框架入门实战\n\n首先，有必要清楚Dubbo是什么。官方文档的定义如下：\n\n\n  DUBBO是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。\n\n\n其实，总结起来就是：\n\n\n  Dubbo是一个解决大规模服务治理的高性能分布式服务框架。\n\n\ndubbo的架构如下：\n\n\n\n节点角色说明：\n\n\nProvider: 暴露服务的服务提供方。\nConsumer: 调用远程服务的服务消费方。\nRegistry: 服务注册与发现的注册中心。\nMonitor: 统计服务的调用次调和调用时间的监控中心。\nContainer: 服务运行容器。\n\n\n调用关系说明：\n\n\n服务容器负责启动，加载，运行服务提供者。\n服务提供者在启动时，向注册中心注册自己提供的服务。\n服务消费者在启动时，向注册中心订阅自己所需的服务。\n注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。\n服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。\n服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。\n\n\nDubbo入门实战\n\n要完成一个dubbo的demo，其实只需要上面的Provider、Consumer和Registry。其中Registry使用Zookeeper来承担，当提供者提供服务后，需要向Zookeeper（注册中心）暴露其发布的服务，消费者通过Zookeeper订阅其需要消费的服务，然后就可以像调用本地服务一样调用远程服务了。\n\n服务提供者：\n\n接口定义：\n\npackage com.rhwayfun.service;\n\n/**\n * \n * @ClassName: HelloService \n * @Description: TODO\n * @author ZhongCB\n * @date 2016年8月1日 下午4:40:09 \n *\n */\npublic interface HelloService {\n\n    String getName();\n}\n\n\n接口实现类：\n\n\n\npackage com.rhwayfun.service.impl;\n\nimport com.rhwayfun.service.HelloService;\n\n/**\n * \n * @ClassName: HelloServiceImpl \n * @Description: TODO\n * @author ZhongCB\n * @date 2016年8月5日 下午5:12:00 \n *\n */\npublic class HelloServiceImpl implements HelloService {\n\n    public String getName() {\n        return \"rhwayfun\";\n    }\n\n}\n\n\nSpring配置文件：\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<beans xmlns=\"http://www.springframework.org/schema/beans\"  \n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://code.alibabatech.com/schema/dubbo\"  \n    xsi:schemaLocation=\"http://www.springframework.org/schema/beans  \n        http://www.springframework.org/schema/beans/spring-beans.xsd  \n        http://code.alibabatech.com/schema/dubbo  \n        http://code.alibabatech.com/schema/dubbo/dubbo.xsd \">   \n\n    <!-- 具体的实现bean -->  \n    <bean id=\"helloService\" class=\"com.rhwayfun.service.impl.HelloServiceImpl\" />  \n    <!-- 提供方应用信息，用于计算依赖关系 -->  \n    <dubbo:application name=\"provider-dubbo\"  />    \n    <!-- 使用zookeeper注册中心暴露服务地址 -->  \n    <dubbo:registry address=\"zookeeper://127.0.0.1:2181\" />     \n    <!-- 用dubbo协议在20880端口暴露服务 -->  \n    <dubbo:protocol name=\"dubbo\" port=\"29014\" />  \n    <!-- 声明需要暴露的服务接口 -->  \n    <dubbo:service interface=\"com.rhwayfun.service.HelloService\" ref=\"helloService\" />  \n</beans> \n\npom.xml文件：\n\n\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>com.rhwayfun</groupId>\n    <artifactId>provider-demo</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <properties>\n        <spring.version>3.2.8.RELEASE</spring.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>dubbo</artifactId>\n            <version>2.5.3</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.springframework</groupId>\n                    <artifactId>spring</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>com.github.sgroschupf</groupId>\n            <artifactId>zkclient</artifactId>\n            <version>0.1</version>\n        </dependency>\n        <!-- spring相关 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-core</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-beans</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-jdbc</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-web</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-webmvc</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-aop</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-tx</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-orm</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context-support</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-test</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-jms</artifactId>\n            <version>${spring.version}</version>\n        </dependency>\n    </dependencies>\n</project>\n\n下面就需要编写测试类来验证服务提供者是否发布成功，由于使用了Zookeeper作为服务注册中心，所以在运行测试代码之前，需要首先启动Zookeeper。具体Zookeeper的使用与配置请参考Zookeeper入门实战。\n\n测试代码：\n\n\n\npackage com.rhwayfun.test;\n\nimport java.io.IOException;\n\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\n/**\n * \n * @ClassName: HelloServiceTest \n * @Description: TODO\n * @author ZhongCB\n * @date 2016年8月5日 下午5:17:52 \n *\n */\npublic class HelloServiceTest {\n\n    public static void main(String[] args) throws IOException{\n        ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(new String[]{\"application.xml\"});\n        ctx.start();\n        System.out.println(\"服务提供者已注册成功！\");\n        System.in.read();\n    }\n}\n\n\n如果服务发布成功，则会在控制台打印出“服务提供者已注册成功！”的提示。\n\n服务消费者：\n\npom.xml配置文件，在服务提供者的pom.xml基础添加如下如下依赖：\n\n\n\n<dependency>\n    <groupId>com.rhwayfun</groupId>\n    <artifactId>provider-demo</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n</dependency>\n\nSpring配置文件：\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://code.alibabatech.com/schema/dubbo\"\n    xsi:schemaLocation=\"http://www.springframework.org/schema/beans  \n        http://www.springframework.org/schema/beans/spring-beans.xsd  \n        http://code.alibabatech.com/schema/dubbo  \n        http://code.alibabatech.com/schema/dubbo/dubbo.xsd \">\n\n    <!-- 服务消费者应用名称，不要与提供者应用名称一致 -->\n    <dubbo:application name=\"consumer-dubbo\" />\n\n    <!-- 使用zookeeper注册中心订阅服务地址 -->  \n    <dubbo:registry address=\"zookeeper://127.0.0.1:2181\" />   \n\n    <!-- 生成远程服务代理，可以和本地bean一样使用HelloService -->\n    <dubbo:reference id=\"helloService\" interface=\"com.rhwayfun.service.HelloService\" />\n</beans>\n\n使用服务提供者发布的服务：\n\n\n\npackage com.rhwayfun.consumer.dubbo;\n\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\nimport com.rhwayfun.service.HelloService;\n\n/**\n * \n * @ClassName: DubboConsumerDemo \n * @Description: TODO\n * @author ZhongCB\n * @date 2016年8月5日 下午4:14:58 \n *\n */\npublic class DubboConsumerDemo {\n\n    public static void main(String[] args) {\n        ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(new String[]{\"classpath:application.xml\"});\n        ctx.start();\n\n        // 获取远程服务代理\n        HelloService helloservice = (HelloService)ctx.getBean(\"helloService\");\n        System.out.println(helloservice.getName());\n    }\n}\n\n\n以上就是使用dubbo的入门实例，我们发现使用dubbo调用远程服务非常方便，感觉调用本地接口没什么很大不同。其实，这就是dubbo的高明之处了，有兴趣可以阅读以下dubbo的源码。\n\n附：dubbo入门实战源码", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/u011116672/article/details/52131362", "title": "Dubbo分布式服务框架入门实战（附源码）"}
{"timestamp": "2018_08_13_17_28_55", "desc": "首先说明一下，我这里讲的是Windows64/32位操作系统下的安装教程。其他linux、Ubuntu环境下的安装暂时还未整理。请自行寻找教程。\n\n安装Scrapy主要分为一下九个步骤： \n1. 安装python。（相信大家都已经安装好了） \n2. 配置python环境变量。（怕大家没有配置，所以这里啰嗦一下） \n3. 下载安装pywin32。 \n4. 下载安装pip和setuptools。（为方便后续使用pip安装scrapy） \n5. 安装Twisted。 \n6. 安装Zope.Interface和pyOpenSSL。 \n7. 安装lxml。 \n8. 下载 Microsoft Visual C++库：VCForPython27.msi。 \n9. 安装scrapy.\n\n接下来我们就来详细讲解每一步。\n\n\n\n1.安装python。\n\n\n官网上选择你想要下载的python版本。\n官网地址：https://www.python.org/downloads/\n或者我这里有python2.7版本的安装包： \nhttp://download.csdn.net/download/mtbaby/9898753直接下载即可。\n下载后直接双击进行安装，建议安装到系统目录下，其余默认。\n比如我安装到C盘，那我安装完后会在C盘下看到C:\\python27.\n\n\n\n\n2.配置环境变量\n\n\n将python的安装目录复制到环境变量。右击“我的电脑”-“属性”-“高级系统设置”-“高级”-“环境变量”，找到“系统变量”里的path，然后将 \nC:\\Python27\\;C:\\Python27\\Scripts;这两个路径添加到后面。\n验证python是否安装成功。打开cmd，输入python， \n \n若没有报错，则安装成功。\n报错则说明你的环境变量加错了。请重新检查。\n\n\n\n\n3.安装pywin32。\n\n\n下载地址 https://sourceforge.net/projects/pywin32/files/pywin32/\n官网上不好找，我这里下载了python2.7的版本的， \nhttp://download.csdn.net/detail/mtbaby/9898883\n你可以直接下载。\n下载后直接双击安装即可，安装完毕之后验证：\n\n在python命令行下输入import win32com\n\n\n\n如果没有提示错误，则证明安装成功\n\n\n\n\n4.安装pip和setuptools。\n\n\nhttps://pypi.python.org/pypi/pip#downloads\n或者http://download.csdn.net/detail/mtbaby/9898910直接下载；\n选择pip-9.0.1.tar.gz，下载后解压，用cmd控制台进入解压目录，输入：\n\npython setup.py install\n\n\n验证安装成功：\n\n\n\n\npip是依赖于setuptools的，安装pip时会自动安装setuptools，所以这里就不赘述了。\n\n\n\n\n5.安装Twisted。\n\n注意：这里Twisted要安装对应版本，Twisted-13.1.0 \n进入python目录，输入命令\n\n\n\npip install twisted==13.1.0\n\n\n\n\n\n6.安装Zope.Interface和pyOpenSSL\n\n1.Zope.Interface下载地址： \n https://pypi.python.org/pypi/zope.interface#downloads \n 2.pyOpenSSL下载地址： \n https://pypi.python.org/pypi/pyOpenSSL#downloads\n\n选择对应python版本的.egg包进行下载。如果你的Python版本和我的一样是2.7的，那这里有我下载好的两个包，直接去拿：\n\nhttp://download.csdn.net/detail/mtbaby/9898934\n\n3.这时候把这两个.egg文件拷贝进入python根目录下的scripts目录，同easy_installs等文件一个目录位置。然后进入cmd模式，在cmd模式下进入这个script目录，执行easy_install.py  egg文件名，执行安装这个egg文件。\n\n如图：\n\n\n\n安装完以后进行验证：\n\n\n\n没有报错说明安装成功。\n\n\n\n7.安装lxml\n\nlxml是一种使用 Python 编写的库，可以迅速、灵活地处理 XML\n\n直接执行如下命令\n\n\n\npip install lxml\n\n就可完成安装，如果提示 Microsoft Visual C++库没安装，则看步骤8.\n\n\n\n8.下载 Microsoft Visual C++库：VCForPython27.msi\n\n下载地址：https://www.microsoft.com/en-us/download/details.aspx?id=44266\n\n下载后双击运行即可。\n\n\n\n9.安装Scrapy\n\n最后就是激动人心的时刻啦，上面的铺垫做好了，我们终于可以享受到胜利的果实啦！\n\n进入python下的lib目录，执行如下命令\n\n\n\n\npip install Scrapy\n\n\n\n安装完后验证：\n\n\n\n如果提示如下命令，就证明安装成功啦，如果失败了，请检查上述步骤有何疏漏。\n\n\n\n安装报错\n\n别看我文章写的安装那么顺利，其实在安装的时候踩了不少坑，下面我就来说说：\n\n1.步骤5，在安装Twisted的时候，我是下载Twisted的版本是Twisted 11.1，结果下载后安装完，进行步骤9的时候，提示\n\n…… \nRequirement already satisfied: Twisted>=13.1.0 in /usr/lib64/python2.7/site-packages (from Scrapy) \n…… \nTLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,  \nAttributeError: ‘module’ object has no attribute ‘OP_NO_TLSv1_1’\n\n网上查了一下，结果是Twisted 的版本不对，需要下载13.1版本的， \n这里附上地址https://twistedmatrix.com/Releases/Twisted/13.1/\n\n真的是，内心一万个***在奔腾。好吧，就就用命令下载\n\n\n\npip install twisted==13.1.0\n\n这样就好了。\n\n2.其他的报错我没记住啊，大家遇到了以后要和我分享啊。", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/MTbaby/article/details/75105752", "title": "python爬虫框架-Scrapy安装详细教程"}
{"timestamp": "2018_08_13_17_28_55", "desc": "环境\n\n\npython3.5\nscrapy1.4.0\n\n\n\n\n代码\n\n\nitems.py\n\n\n\n\n# -*- coding: utf-8 -*-\n\nimport scrapy\n\nclass MeiziSpiderItem(scrapy.Item):\n    image_url = scrapy.Field()   # 存放图片真实的URL\n    refer_url = scrapy.Field()   # 存放图片下载时对应的请求Refer\n\n\nspiders/meizi.py\n\n\n# -*- coding: utf-8 -*-\n\nfrom scrapy.spider import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nfrom MeiziSpider.items import MeiziSpiderItem\n\n\nclass MeiziSpider(CrawlSpider):\n    name = 'meizi'\n    allowed_domains = ['www.mzitu.com']\n    start_urls = ['http://www.mzitu.com/']\n\n    rules = (\n        Rule(LinkExtractor(allow=('http://www.mzitu.com/\\d{1,6}',)), callback='parse_item', follow=True),\n        Rule(LinkExtractor(allow=('http://www.mzitu.com/\\d{1,6}/\\d{1,3}',)), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        img_item = MeiziSpiderItem()\n        img_item['image_url'] = response.css(\".main-image p a img::attr(src)\").extract()\n        img_item['refer_url'] = response.url\n\n        yield img_item\n\n\npipelines.py\n\n\n\n\n# -*- coding: utf-8 -*-\n\nfrom scrapy.contrib.pipeline.images import ImagesPipeline\nfrom scrapy.http import Request\n\n\nclass MyImagesPipeline(ImagesPipeline):\n\n    def get_media_requests(self, item, info):\n        for image_url in item['image_url']:\n            default_headers = {\n                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36',\n                'referer': '{}'.format(item['refer_url'])\n            }\n            yield Request(image_url, headers=default_headers)\n\n\n\n    def item_completed(self, results, item, info):\n        return item\n\n\nsettings.py\n\n\n\n\n...\nROBOTSTXT_OBEY = False\nITEM_PIPELINES = {\n   'MeiziSpider.pipelines.MyImagesPipeline': 5,\n}\nIMAGES_URLS_FIELD =\"image_url\"  #image_url是在items.py中配置的网络爬取得图片地址\n#配置保存本地的地址\nproject_dir = os.path.abspath(os.path.dirname(__file__))  #获取当前爬虫项目的绝对路径\nIMAGES_STORE = os.path.join(project_dir, 'images')  #组装新的图片路径\n...\n\n\n\n效果\n\n\n共抓取到3G多的图片，花费1.5小时", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/ns2250225/article/details/78235406", "title": "【Python】妹子图图片全站抓取"}
{"timestamp": "2018_08_13_17_28_55", "desc": "在官网上下载apache-tomcat-7.0.75.tar.gz、jdk-8u121-linux-x64.tar.gz\n将Tomcat放在home文件夹下\n将jdk放在usr/java文件夹下，java文件夹自己新建\n\n\n\n\n\nmysql安装\n安装mysql客户端–>yum install mysql\n安装mysql服务器端–>yum install mariadb-server   yum install mysql-devel\n安装完mysql后，在mysql的配置文件/etc/my.cnf文件头中加入character-set-server=utf8 \n启动mysql,若安装mariadb服务器端，执行service mariadb start\n查看mysql是否开启 \n    netstat -tln \n-进入mysql修改密码 \nmysql -uroot  -p \nset password for 用户名@localhost = password(‘新密码’);         \n\n\n\n\n\n配置java,修改/etc/profile文件\n在末尾追加\nJAVA_HOME=/usr/java/jdk1.8.0_121\nJAVA_BIN=/usr/java/jdk1.8.0_121/bin\nPATH=PATH:$JAVA_BIN\nCLASSPATH=美元符JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nexport JAVA_HOME JAVA_BIN PATH CLASSPATH\n使配置生效  –># source /etc/profile", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/wellxielong/article/details/79179643", "title": "Linux服务器下部署javaweb项目环境"}
{"timestamp": "2018_08_13_17_28_55", "desc": "今天给大家分享一个小网站的数据采集，并写到excel里面！\n\n\n\n\n\n分析网站\n\n目标网站是“小咪购”，这里有天猫所有的含有购物券的商品信息，我们今天就来抓它吧！\n\n\n\n随便找一段文字，然后点击右键查看网页源代码，看看是否存在该文字，如果存在，那么这个网页就是静态网站了！很幸运，这个网站居然是静态的。\n\n\n\n那就简单了，不需要去分析ajax加载数据或者找json包了，直接获取网页源代码==>>匹配相关内容==>>保存数据即可！\n\n\n\n\n\n工具和库\n\nWindows+python3.6\n\nimport random\n\nimport time\n\nimport requests\n\nfrom lxml import etree\n\nimport xlwt\n\n用这几个库就可以搞定了！注意xlwt和xlrd这2个库都是操作excel的，一个是保存数据，一个是读取数据，不要搞混了。\n\n\n\n\n\n开始写代码\n\n首先写一个函数，将所有的爬虫工作写到函数里，如下图\n\n\n\n这个网站需要写上headers，不写是抓不到数据的！新建一个列表，将爬到的相关数据写入列表，它的形式大概是这样的：【【产品信息A1，2，3……】，【产品信息B1，2，3……】……】，这么写列表是因为我们最后要将他们写如excel表格，那么列表中的每一个元素（还是列表形式）都是一行数据，方便写入！\n\n注意第33行，列表构成的时候，用+连接会将所有列表中的元素放入一个列表，比如：【1，2，3】+【4，5】=【1，2，3，4，5】，而用append()函数则会将后面的内容作为一个元素加入列表中，比如：[1,2,3].append([4,5])=[1,2,3,[4,5]]\n\n下来就是写入excel了，首先是新建excel表格，并写入第一行数据\n\n\n\nwb = xlwt.Workbook(encoding='utf-8')\n    ws = wb.add_sheet('优惠券')\n    path = 'E://python//优惠券.xls'\n    head = ['序号','商品类型','商品名称','优惠券','券后价','在售价','销量','推荐理由','商品链接']\n    for i in range(9):\n        ws.write(0,i,head[i])\n\n后面的数据，依次按格式写入并最后用wb.save(路径)的方式保存即可！完整代码及效果如下\n\nimport random\nimport time\nimport requests\nfrom lxml import etree\nimport xlwt\n\ndef main(line = 1):\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0'}\n    url = 'http://www.hlxns.com'\n    html = requests.get(url, headers=headers)\n    html.encoding = 'utf-8'\n    page = etree.HTML(html.text).xpath('//a[@class=\"item\"]/text()')[-1]\n    for i in range(int(line/100)+1, int(page) + 1):\n        k = 1\n        lis = []\n        print('【开始下载】第%d页数据'%i)\n        htm = requests.get(url + '/index.php?r=l&page={}'.format(str(i)), headers=headers)\n        htm.encoding = 'utf-8'\n        data = etree.HTML(htm.text)\n        url_sps = data.xpath('//div[@class=\"title\"]/a/@href')\n        for url_sp in url_sps:#一页100条\n            time.sleep(random.random()*2)\n            print('      【正在下载】第%03d页第%03d条商品数据'%(i,k),end='')\n            k += 1\n            html_sp = requests.get(url + url_sp, headers=headers)\n            html_sp.encoding = 'utf-8'\n            info = etree.HTML(html_sp.text)\n            title = info.xpath('//span[@class=\"title\"]/text()')  # 产品\n            summary = [x.replace('推荐理由：','') for x in  info.xpath('//span[@class=\"theme-color-3\"]/text()')]  # 推荐理由\n            category = info.xpath('//div[@class=\"nav-wrap\"]/div/a[3]/text()')  # 类别\n            now_price = info.xpath('//span[@class=\"now-price\"]/b[2]/i/text()')  # 券后价\n            old_price = info.xpath('//span[@class=\"org-price\"]/i/text()')  # 在售价\n            nums = info.xpath('//div[@class=\"text-wrap\"]/span[2]/i/text()')  # 销量\n            coupon = info.xpath('//div[@class=\"buy-coupon theme-color-8\"]/span/b/text()')  # 优惠券\n            sp_url = info.xpath('//a[@class=\"theme-bg-color-8\"]/@href')  # 链接\n            lis.append(category+title+coupon+now_price+old_price+nums+summary+sp_url)\n            print('................................【下载完成】')\n        print('######第%d页数据   【下载完成】'%i)\n        for ii in range(len(lis)):\n            lis[ii].insert(0, line)  # 添加序号\n            for j in range(9):  # 列\n                ws.write(line, j, lis[ii][j])\n            line += 1\n        print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>【写入本页数据完成】<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n        wb.save(path)\nif __name__ == '__main__':\n    wb = xlwt.Workbook(encoding='utf-8')\n    ws = wb.add_sheet('优惠券')\n    path = 'E://python//优惠券.xls'\n    head = ['序号','商品类型','商品名称','优惠券','券后价','在售价','销量','推荐理由','商品链接']\n    for i in range(9):\n        ws.write(0,i,head[i])\n    main()\n\n\n\n\n\n由于网站更新的很快（官方说是10分钟。。。），所以也没有抓取那么多，所有的页面有大约600多页，一页100条信息，也就是说一共有6万多条商品信息，如果不用多线程的话会很慢！", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/programmer_yf/article/details/81185747", "title": "python简单应用！用爬虫来采集天猫所有优惠券信息，写入本地文件"}
{"timestamp": "2018_08_13_17_28_55", "desc": "好吧，我又开始折腾豆瓣电影top250了，只是想试试各种方法，看看哪一种的方法效率是最好的，一直进行到这一步才知道 scrapy的强大，尤其是和selector结合之后，速度飞起。。。。 \n下面我就采用scrapy爬取豆瓣电影top250的方法记录一下，里面采用了selector的xpath和css两种各实现了一遍，其中xpath的选取路径较为清晰以及能够直接选到标签属性，css则较为有点小抽象，没有xpath直观，优点是结构简单，书写方便。于是我都实现了一遍，汗哒哒。。。\n\n\n\n步骤一：生成项目名称\n\n从windows的cmd命令行或者ubuntu的终端下进入你想保存项目的文件下，输入一下代码：\n\n\n\nscrapy startproject doubanmovie (注意：换成你的项目名称)\ncd doubanmovie\n\ncmd中进入doubanmovie后，也就是进入你的项目文件夹后，后面所有的在cmd下或者终端下进行的scrapy命令都是在这个目录下进行的。\n\n\n\n步骤二：配置settings.py文件\n\n这个文件就是一些基本的配置，比如访问网站的延迟代理等等，在这里我们设置下将爬取的数据以指定的格式保存到指定目录\n\n\n\nFEED_URI = u'file:///E:/python/wswp-code/MyExample/MyScrapy/douban3.csv'  #将抓取的数据存放到douban.csv文件中。(从E:/开始，换成你的目录)\nFEED_FORMAT = 'CSV'\n\n然后就是关闭代理了，默认使用代理，有些网页使用本地代理无法访问，否则会报错[scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023 \n [scrapy] DEBUG: Crawled (403)HTTP status code is not handled or not allowed \n 此时，修改setting.py如下：\n\n\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n}\n\n之后就可以正常爬取豆瓣电影网站了。\n\n\n\n步骤三：配置items.py文件\n\n\n\nimport scrapy\nclass Doubanmovie3Item(scrapy.Item):\n    rank = scrapy.Field()#电影排名\n    title = scrapy.Field()#标题--电影名\n    link = scrapy.Field()#详情链接\n    star = scrapy.Field()#电影评分\n    rate = scrapy.Field()#评价人数\n    quote = scrapy.Field() #名句\n\n\n\n步骤四：生成spider文件夹下的爬虫douban.py文件\n\n在上面的cmd下或者终端下：\n\n\n\nscrapy genspider douban \n\n或者\n\n\n\nscrapy genspider douban  movie.douban.com \n\n\n\n步骤五：配置spider文件夹下的douban.py文件\n\n这里也就是重点了，爬取的规则和解析都在这里进行书写，我在这里采用了selector的xpath和css方法分别实现了一遍，就是想进行个对比，读者可以随便选择一个。 \nxpath方法：\n\n\n\n# -*- coding: utf-8 -*-\nimport scrapy\n\nfrom doubanmovie.items import DoubanmovieItem\nclass DoubanSpider(scrapy.Spider):\n    name = \"douban\"\n    allowed_domains = [\"movie.douban.com\"]\n    start_urls = ['https://movie.douban.com/top250']\n\n    def parse(self, response):\n        for info in response.xpath('//div[@class=\"item\"]'):\n            item = DoubanmovieItem()\n            item['rank'] = info.xpath('div[@class=\"pic\"]/em/text()').extract()\n            item['title'] = info.xpath('div[@class=\"pic\"]/a/img/@alt').extract()\n            item['link'] = info.xpath('div[@class=\"pic\"]/a/@href').extract()\n            item['star'] = info.xpath('div[@class=\"info\"]/div[@class=\"bd\"]/div[@class=\"star\"]/span[@class=\"rating_num\"]/text()').extract()\n            item['rate'] = info.xpath('div[@class=\"info\"]/div[@class=\"bd\"]/div[@class=\"star\"]/span[4]/text()').extract()\n            item['quote'] = info.xpath('div[@class=\"info\"]/div[@class=\"bd\"]/p[@class=\"quote\"]/span/text()').extract()\n            yield item\n\n            # 翻页\n            next_page = response.xpath('//span[@class=\"next\"]/a/@href')\n            if next_page:\n                url = response.urljoin(next_page[0].extract())\n                yield scrapy.Request(url, self.parse)  \n\ncss方法：\n\n\n\n# -*- coding: utf-8 -*-\nimport scrapy\n\nfrom doubanmovie.items import DoubanmovieItem\nclass DoubanSpider(scrapy.Spider):\n    name = \"douban\"\n    allowed_domains = [\"movie.douban.com\"]\n    start_urls = ['http://movie.douban.com/top250']\n\n    def parse(self, response):\n        for info in response.css('div.item'):\n            item = DoubanmovieItem()\n            item['rank'] = info.css('div.pic em::text').extract()\n            item['title'] = info.css('div.pic a img::attr(alt)').extract()\n            item['link'] = info.css('div.pic a::attr(href)').extract()\n            item['star'] = info.css('div.info div.bd div.star span.rating_num::text').extract()\n            rate = info.css('div.info div.bd div.star span')\n            item['rate'] = rate[3].css('::text').extract()\n            item['quote'] = info.css('div.info div.bd p.quote span::text').extract()\n            yield item\n\n            # 翻页\n            next_page = response.xpath('//span[@class=\"next\"]/a/@href')\n            if next_page:\n                url = response.urljoin(next_page[0].extract())\n                yield scrapy.Request(url, self.parse)   \n\"\"\"\n补充点关于css的用法\nScrapy的中Css 选择器\n\n//通过 名为 video_part_lists 的Class 中下面的 li 标签\n\nliList = response.css('.video_part_lists li') \n\nfor li in liList:\n再通过li 标签获取 a 标签中得  href 属性\n    name= li.css('a::attr(href)').extract()\n\"\"\"\n\n\n\n步骤六：启动爬虫，观察日志\n\n在上面的cmd中或者终端下：\n\n\n\nscrapy crawl douban -s LOG_LEVEL=INFO\n\n或者\n\n\n\nscrapy crawl douban -s LOG_LEVEL=DEBUG\n\n这个设置可以观察到详细的运行日志\n\n\n\n步骤七：解决乱码问题\n\n此时在你之前设定的保存文件的目录下，有了一个.csv文件，用window的记事本或者notepad++打开的话，可以正常显示；但是用excel打开的话则可能出来乱码，此时只需要重新用记事本打开然后另存为ANSI编码，然后就能用window的excel打开了。\n\n附上参考的链接： \nhttp://www.jianshu.com/p/62e0a588ee0d \nhttp://www.tuicool.com/articles/Un2MNfe \nhttp://blog.csdn.net/heu07111121/article/details/50832999  \nhttp://www.tuicool.com/articles/eymema", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/circle2015/article/details/53053632", "title": "Scrapy中用xpath/css爬取豆瓣电影Top250：解决403HTTP status code is not handled or not allowed"}
{"timestamp": "2018_08_13_17_28_55", "desc": "# encoding: utf-8\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\nimport numpy as np\nfrom keras.datasets import mnist\nimport gc\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.applications.vgg16 import VGG16\nfrom keras.optimizers import SGD\n\n\nimport cv2\nimport h5py as h5py \nimport numpy as np\n\n\n##由于输入层需要10个节点，所以最好把目标数字0-9做成one Hot编码的形式。\ndef tran_y(y): \n    y_ohe = np.zeros(10) \n    y_ohe[y] = 1 \n    return y_ohe\n\n\n# 如果硬件配置较高，比如主机具备32GB以上内存，GPU具备8GB以上显存，可以适当增大这个值。VGG要求至少48像素\nishape=48\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = [cv2.cvtColor(cv2.resize(i, (ishape, ishape)), cv2.COLOR_GRAY2BGR) for i in X_train] \nX_train = np.concatenate([arr[np.newaxis] for arr in X_train]).astype('float32') \nX_train /= 255.0\n\nX_test = [cv2.cvtColor(cv2.resize(i, (ishape, ishape)), cv2.COLOR_GRAY2BGR) for i in X_test] \nX_test = np.concatenate([arr[np.newaxis] for arr in X_test]).astype('float32')\nX_test /= 255.0\n\ny_train_ohe = np.array([tran_y(y_train[i]) for i in range(len(y_train))]) \ny_test_ohe = np.array([tran_y(y_test[i]) for i in range(len(y_test))])\ny_train_ohe = y_train_ohe.astype('float32')\ny_test_ohe = y_test_ohe.astype('float32')\n\n\nprint X_train.shape\n\n# VGG16 全参重训迁移学习\n\n# 很多时候需要多次回收垃圾才能彻底收回内存。如果不行，重新启动单独执行下面的模型\nfor i in range(10):\n    gc.collect()\n\n\n\nishape=224\nmodel_vgg = VGG16(include_top = False, weights = 'imagenet', input_shape = (ishape, ishape, 3)) \n\nfor layer in model_vgg.layers:\n        layer.trainable = False\nmodel = Flatten()(model_vgg.output) \nmodel = Dense(4096, activation='relu', name='fc1')(model)\nmodel = Dense(4096, activation='relu', name='fc2')(model)\nmodel = Dropout(0.5)(model)\nmodel = Dense(10, activation = 'softmax', name='prediction')(model) \nmodel_vgg_mnist_pretrain = Model(model_vgg.input, model, name = 'vgg16_pretrain')\nprint model_vgg_mnist_pretrain.summary()\n\n\n##我们只需要训练25万个参数，比之前整数少了60倍。\nsgd = SGD(lr = 0.05, decay = 1e-5) \nmodel_vgg_mnist_pretrain.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n\nmodel_vgg_mnist_pretrain.fit(X_train, y_train_ohe, validation_data = (X_test, y_test_ohe), epochs = 10, batch_size = 64)\n\n\n#######在测试集上评价模型精确度\nscores=model_vgg_mnist_pretrain.evaluate(X_test,y_test_ohe,verbose=0)\n\n#####打印精确度\nprint scores\n\n\ntime2 = time.time()\nprint u'ok,结束!'\nprint u'总共耗时：' + str(time2 - time1) + 's'", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/u013421629/article/details/79296499", "title": "【python keras实战】利用VGG卷积神经网络进行手写字体识别"}
{"timestamp": "2018_08_13_17_28_55", "desc": "那天有朋友找我一起抓妹子图片，像我这么正派的人，肯定是要抱着学习的目的……打开网页，哇！又…又要上火了。。。。\n\n\n\n话不多说，先看看网页吧！\n\n\n\n熟练的打开网页，找到妹子图，然后F12打开开发者工具，定位，ok，图片地址找到了，so easy~!，我们写入代码看看\n\n\n\nWTF~!\n\n返回了个空列表？加了header也是这样，为啥呢，来打开源码看看\n\n\n\n原来是js数据，这就麻烦了，难道先去学习下js语法，拿js文件，分析逻辑，找最终的url吗？\n\n\n\n​让我们用selenium+PhantomJS试试吧！\n\n安装请自行百度（有些麻烦，需要下载各种浏览器的相关驱动文件，网上一堆教程）\n\n我们先获取源码后找找刚才的标签看看效果！\n\n\n\n​吼吼~乖乖的出现了吧！\n\n下面的流程就简单了，找到URL中翻页按钮，循环它，直到没有为止！然后写个获取图片并下载的函数，不到40行代码搞定！剩下的就是静静等待了！\n\n\n\n完整代码和下载效果如下：\n\n\n\n\n\n\n\n\n\n谢谢观看，如有问题可以联系博主哦！直接私信就可以!", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/programmer_yf/article/details/80554347", "title": "简单应用Selenium+PhantomJS来抓取煎蛋网妹子图"}
{"timestamp": "2018_08_13_17_28_55", "desc": "ZooKeeper 简介\n\nZooKeeper 是 Apache 软件基金会的一个软件项目，他为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。 \nZooKeeper 曾经是 Hadoop 的一个子项目，但现在是一个独立的顶级项目。(from wiki)\n\n本文将要介绍的是将 ZooKeeper 作为 Dubbo 的注册中心 \n\n\n需要注意的是：\n\n\n注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小\n注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外\n注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者\n注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表\n注册中心和监控中心都是可选的，服务消费者可以直连服务提供者\n\n\n\n\n安装 ZooKeeper\n\n首先在 ZooKeeper 官网下载最新版的 ZooKeeper，一个压缩包解压缩之后目录结构是下面这个样子的\n\n\nbin \n可执行脚本\nconf \n配置（生效的配置文件为 zoo.cfg）\nlib \n依赖包\ncontrib \n一些用于操作zk的工具包\nrecipes \nzk某些用法的代码示例\n\n\n然后将 /conf/zoo_sample.cfg 重命名为 zoo.cfg 或者新建一个 zoo.cfg 文件\n\n修改 zoo.cfg 文件里面的 dataDir 为 dataDir=D:/zookeeper-3.4.13/server-data，下面贴一下这边完整的配置项\n\n\n\n# CS通信心跳时间\ntickTime=2000\n\n# 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。\ninitLimit=10\n\n# 集群中flower服务器（F）跟leader（L）服务器之间的请求和答应最多能容忍的心跳数。\nsyncLimit=5\n\n# 该属性对应的目录是用来存放myid信息跟一些版本，日志，跟服务器唯一的ID信息等。\ndataDir=D:/Program Files/zookeeper-3.4.13/zookeeper\n\n# 端口\nclientPort=2181\n\n改好了之后切换至 /bin 目录下，这里面有很多脚本，我们在 win 环境下就运行 zkServer.cmd 就好了， linux 环境下应该运行 zkServer.sh (没有实际测试这个)。\n\n\n\n像这样， ZooKeeper 就顺利的跑起来了。\n\n\n\n替换 Dubbo 的注册中心\n\n这里我们将上一篇中的 Provider 程序和 Consumer 程序中的配置项 <dubbo:registry address=\"multicast://224.5.6.7:1234\"/> 关于注册中心的地址更改为 zookeeper://providerhost:2181, 由于这里在本机测试所以这里就直接改为 zookeeper://127.0.0.1:2181 即可，如果你是在局域网内的不同主机上使用这里需要更改为 Provider 的 ip 地址。\n\n添加上操作 zookeeper 相关的依赖\n\n<!-- zookeeper Client-->\n<dependency>\n    <groupId>com.101tec</groupId>\n    <artifactId>zkclient</artifactId>\n    <version>0.10</version>\n    <exclusions>\n        <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n        </exclusion>\n        <exclusion>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n\n<!-- 操作 zookeeper -->\n<dependency>\n    <groupId>org.apache.curator</groupId>\n    <artifactId>curator-framework</artifactId>\n    <version>4.0.0</version>\n</dependency>\n\n然后先启动 Provider, 再启动 Consumer 即可\n\n通过 SpringBoot 启动 Dubbo\n\nhttps://github.com/xiaop1ng/PlayWithSpringBoot\n\n这里提供一个通过 SpringBoot 启动 Dubbo 的方式，感兴趣的同学可以研究研究。 \n使用 @ImportResource 注解来加载配置文件\n\n@Configuration\n//扫描 所有需要的包, 包含一些自用的工具类包 所在的路径\n@ComponentScan(basePackages= {\"com.xiaoping\"})\n@ImportResource(\"dubbo-provider.xml\")\npublic class AppConfig {}", "time": "2018_08_13_17_28_55", "link": "https://blog.csdn.net/xiaoping0915/article/details/81223051", "title": "分布式服务框架 Dubbo 使用 ZooKeeper 作为注册中心（二）"}
{"timestamp": "2018_08_13_17_28_56", "desc": "Spring boot依从与COC原则（convention over configuration：约定优于配置），被设计用来快速实施spring应用。 \n  在本系列教程中，我们将会从一个一个简单的实例逐渐了解spring boot从而能够利用其进行自由的开发设计。本文将会介绍一下环境的搭建以及一个最简单的groovy的helloworld来入门。\n\n\n\n\nSpring boot\n\nspring boot是spring诸多project中的一个，他目的在于进一步简化框架对开发者的负担，使得开发者能够更加快速的开发，更能专注于业务逻辑的实现。它使得创建一个独立运行的spring应用无比简单，更是与当前微服务的设计与架构无比紧密地联系在一起。\n\n\n\n特性\n\n. 创建独立运行的spring应用 \n. 内置Tomcat,Jetty或Undertow（无需部署WAR文件） \n. 提供starter的pom以简化maven设定 \n. 自动配置spring \n. 提供生产环境所需要的诸如Health check等特性 \n. 无需繁琐的XML配置\n\n\n\nSpring boot CLI\n\nCLI是Command Line Interface缩写，使用Sprint Boot CLI可以使得用于以一种及其简单的方式开始spring的项目。\n\n\n\n安装\n\n安装：http://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-installing-spring-boot.html \n在章节 10.2.1 Manual installation 可以直接下载所需要的压缩包 \n解压 \n将cli的bin目录添加到PATH中 \n确认\n\n\n\nC:\\>spring --version\nSpring CLI v1.4.1.RELEASE\nC:\\>\n\n\n\nFirst Helloworld\n\n创建一个application.groovy文件，具体内容如下：\n\n\n\n@RestController\nclass HelloWorld{\n  @RequestMapping(\"/\")\n  def sayhello(){\n    return \"hello liumiaocn\"\n  }\n}\n\n\n\n运行\n\n\n  spring application.groovy \n  运行的时候，spring boot会自动地为以上所写的几行程序创建spring mvc框架同时启动JVM和一个内嵌的服务器，在8080口打开服务。\n\n\n\n\n确认结果\n\nURL: http://localhost:8080 \n通过此URL即可确认返回的结果。这样我们甚至都没有看到java的main函数，没有设定tomcat，没有修改web.xml第一个demo的helloworld就在几分钟内草率地被结束了， \n从某个角度来说也这也印证了spring boot的强大，这是这个浮躁而快速变化的时代所急需的功能。", "time": "2018_08_13_17_28_56", "link": "https://blog.csdn.net/liumiaocn/article/details/53431149", "title": "Spring基础：快速入门spring boot（1）：CLI方式的Helloworld"}
{"timestamp": "2018_08_13_17_28_56", "desc": "写博客，一部分是为了让自己今后能快速地复习之前学过的知识，整理下思路；另一方面是帮助到其他也遇到类似问题的童鞋。但是写博客很难坚持下来，原因嘛，各种各样。但说到底是没有“共鸣”。\n\n高山流水，难觅知音。\n\n其实，建立起写博客的习惯，就是那些点点滴滴的小事：每天看着博客的访问量，点赞数增加；看到自己的文章被别人评论等等。\n\n\n\n好了，废话不多说。今天来谈谈关于刷浏览量的问题。虽然这远远的偏离了写博客的初衷，但是了解下这类问题还是不错的，毕竟“技术并不犯法！”。\n\n\n\n反（反）爬虫机制\n\n说到反爬虫，不得不说下爬虫了。其实这就是一个概念，爬虫就是将手动完成的事情交给了代码去自动化的实现罢了。而反爬虫就是探查用户是真实用户还是代码的一种手段。而反反爬虫就是针对反爬虫机制的一种手段。\n\n都说“双重否定，表示肯定”，那么爬虫和反反爬虫应该是一样的了。其实不然，表面上行为是一致的，但是实际上反反爬虫做了更多的处理，而不是简单的小爬虫啦。\n\n大体上来讲，反爬虫会从如下几个层面入手： \n- header 浏览器的请求头 \n  - User-Agent 用户代理，表明访问源身份的一种方式 \n  - Referer    访问的目标链接是从哪个链接跳转过来的（做防盗链的话，就可以从它入手） \n  - Host  同源地址判断，用它会很有用 \n- IP 同一个IP短时多次访问，就很有可能是爬虫，反爬虫会对此做处理。 \n- 访问频率：短时多次高并发的访问，基本上就是有问题的访问。 \n上面这几个都是常见的反爬虫措施，当然还有更加高深的机制，比如最恶心的验证码（使用tesseract可以处理较为简单的验证码识别），用户行为分析，等等等等。\n\n既然了解了常见的反爬虫机制，那相对应的进行“政策-对策”实现反反爬虫也就不是那么的没有头绪了。是的，针对上面的限制，我们会有一些对策。\n\n\n针对User-Agent 的，可以整理一些常见的浏览器代理头，每次访问随机使用其中一个就好了。\n针对IP的，可以使用代理IP嘛\n针对频率限制的，做下访问间隙做下随机休眠就挺不错的。\n……\n\n\n\n\n\n\n实战\n\n之前我一直是在CSDN上写博客，它的反爬虫机制说实话，做的比较的浅，一方面必要性不是很大，二来做反爬虫经纪上不太划算，估计他们也不愿意在这上面浪费吧。\n\n所以，在CSDN上刷浏览量还是很随意的，说下我的思路。 \n- 代理IP爬取，验证清洗数据，定期更新。 \n- 浏览器User-Agent整理，添加访问的随机性。 \n- 随即休眠策略，日志处理，错误记录，定时重试等。\n\n\n\n代理IP处理\n\n\n\n# coding: utf8\n\n# @Author: 郭 璞\n# @File: proxyip.py                                                                 \n# @Time: 2017/10/5                                   \n# @Contact: 1064319632@qq.com\n# @blog: http://blog.csdn.net/marksinoberg\n# @Description: 抓取代理IP，并保存到redis相关的key中\nimport requests\nfrom bs4 import BeautifulSoup\nfrom redishelper import RedisHelper\n\nclass ProxyIP(object):\n    \"\"\"\n    抓取代理IP，清洗，验证。\n    \"\"\"\n    def __init__(self):\n        self.rh = RedisHelper()\n\n    def crawl(self):\n        \"\"\"\n        不管是http还是https统统存进去再说。\n        \"\"\"\n        # 先处理http模式的代理ip\n        httpurl = \"http://www.xicidaili.com/nn/\"\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n        }\n        html = requests.get(url=httpurl, headers=headers).text\n        soup = BeautifulSoup(html, \"html.parser\")\n        ips = soup.find_all(\"tr\")\n        for index in range(1, len(ips)):\n            tds = ips[index].find_all('td')\n            ip = tds[1].text\n            port = tds[2].text\n            ipinfo = \"{}:{}\".format(ip, port)\n            if self._check(ip):\n                self.rh.sAddAvalibeIp(ipinfo)\n            # print(ipinfo)\n\n\n\n\n    def _check(self, ip):\n        \"\"\"\n        检测代理IP的有效性\n        \"\"\"\n        checkurl = \"http://47.94.19.186/common/checkip.php\"\n        localip = self._getLocalIp()\n        # print(\"Local: {}, proxy: {}\".format(localip, ip))\n        return False if localip==ip else True\n\n    def _getLocalIp(self):\n        \"\"\"\n        获取本机的IP地址, 接口方式不太靠谱，暂时用手工方式在https://www.baidu.com/s?ie=UTF-8&wd=ip 进行手动复制粘贴即可\n        \"\"\"\n        return \"223.91.239.159\"\n\n    def clean(self):\n        ips = self.rh.sGetAllAvalibleIps()\n        for ipinfo in ips:\n            ip, port = ipinfo.split(\":\")\n            if self._check(ip):\n                self.rh.sAddAvalibeIp(ipinfo)\n            else:\n                self.rh.sRemoveAvalibleIp(ipinfo)\n\n    def update(self):\n        pass\n\n\nif __name__ == '__main__':\n    pip = ProxyIP()\n    # result = pip._check(\"223.91.239.159\", 53281)\n    # print(result)\n    pip.crawl()\n    # pip.clean()\n\n\n\nRedis工具类\n\n\n\n# coding: utf8\n\n# @Author: 郭 璞\n# @File: redishelper.py                                                                 \n# @Time: 2017/10/5                                   \n# @Contact: 1064319632@qq.com\n# @blog: http://blog.csdn.net/marksinoberg\n# @Description: 涉及redis的一些操作工具方法\n\nimport redis\n\nclass RedisHelper(object):\n    \"\"\"\n    用于保存爬取到的博客内容链接。\n    保存代理IP\n    \"\"\"\n    def __init__(self):\n        self.articlepool = \"redis:set:article:pool\"\n        self.avalibleips = \"redis:set:avalible:ips\"\n        self.unavalibleips = \"redis:set:unavalibe:ips\"\n\n        pool = redis.ConnectionPool(host=\"localhost\", port=6379)\n        self.redispool = redis.Redis(connection_pool=pool)\n\n    def sAddArticleId(self, articleid):\n        \"\"\"\n        添加爬取到的博客id。\n        :param articleid:\n        :return:\n        \"\"\"\n        self.redispool.sadd(self.articlepool, articleid)\n\n    def sRemoveArticleId(self, articleid):\n        self.redispool.srem(self.articlepool, articleid)\n\n    def popupArticleId(self):\n        return int(self.redispool.srandmember(self.articlepool))\n\n    def sAddAvalibeIp(self, ip):\n        self.redispool.sadd(self.avalibleips, ip)\n    def sRemoveAvalibeIp(self, ip):\n        self.redispool.srem(self.avalibleips, ip)\n    def sGetAllAvalibleIps(self):\n        return [ip.decode('utf8') for ip in self.redispool.smembers(self.avalibleips)]\n\n    def popupAvalibeIp(self):\n        return self.redispool.srandmember(self.avalibleips)\n\n    def sAddUnavalibeIp(self, ip):\n        self.redispool.sadd(self.unavalibleips, ip)\n    def sRemoveUnavaibleIp(self, ip):\n        self.redispool.srem(self.unavalibleips, ip)\n\n\n\n\n\n\ncsdn博文工具类\n\n\n\n# coding: utf8\n\n# @Author: 郭 璞\n# @File: csdn.py                                                                 \n# @Time: 2017/10/5                                   \n# @Contact: 1064319632@qq.com\n# @blog: http://blog.csdn.net/marksinoberg\n# @Description: 爬取一个博主的全部博客链接工具类以及其他设计到的操作。\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\nclass BlogScanner(object):\n    \"\"\"\n    抓取博主id下的所有文章链接id。\n    \"\"\"\n    def __init__(self, bloger=\"marksinoberg\"):\n        self.bloger = bloger\n        # self.blogpagelink = \"http://blog.csdn.net/{}/article/list/{}\".format(self.bloger, 1)\n\n    def _getTotalPages(self):\n        blogpagelink = \"http://blog.csdn.net/{}/article/list/{}?viewmode=contents\".format(self.bloger, 1)\n        html = requests.get(url=blogpagelink).text\n        soup = BeautifulSoup(html, \"html.parser\")\n        # 比较hack的操作，实际开发还是不要这么随意的好\n        temptext = soup.find('div', {\"class\": \"pagelist\"}).find(\"span\").get_text()\n        restr = re.findall(re.compile(\"(\\d+).*?(\\d+)\"), temptext)\n        # print(restr)\n        pages = restr[0][-1]\n        return pages\n\n    def _parsePage(self, pagenumber):\n        blogpagelink = \"http://blog.csdn.net/{}/article/list/{}?viewmode=contents\".format(self.bloger, int(pagenumber))\n        html = requests.get(url=blogpagelink).text\n        soup = BeautifulSoup(html, \"html.parser\")\n        links = soup.find(\"div\", {\"id\": \"article_list\"}).find_all(\"span\", {\"class\": \"link_title\"})\n        articleids = []\n        for link in links:\n            temp = link.find(\"a\").attrs['href']\n            articleids.append(temp.split(\"/\")[-1])\n        # print(len(articleids))\n        # print(articleids)\n        return articleids\n\n    def get_all_articleids(self):\n        pages = int(self._getTotalPages())\n        articleids = []\n        for index in range(pages):\n            tempids = self._parsePage(int(index+1))\n            articleids.extend(tempids)\n        return articleids\n\n\n\nif __name__ == '__main__':\n    bs = BlogScanner(bloger=\"marksinoberg\")\n    # print(bs._getTotalPages())\n    # bs._parsePage(1)\n    articleids = bs.get_all_articleids()\n    print(len(articleids))\n    print(articleids)\n\n\n\nBrush工具类\n\n\n\n# coding: utf8\n\n# @Author: 郭 璞\n# @File: brushhelper.py                                                                 \n# @Time: 2017/10/5                                   \n# @Contact: 1064319632@qq.com\n# @blog: http://blog.csdn.net/marksinoberg\n# @Description: 开刷\n\nimport requests\nimport random\nimport time\nfrom redishelper import RedisHelper\n\n\nclass FakeUserAgent(object):\n    \"\"\"\n    搜集到的一些User-Agent，每次popup出不同的ua，减少反爬虫机制的影响。\n    更多内容：http://www.73207.com/useragent\n    \"\"\"\n\n    def __init__(self):\n        self.uas = [\n            \"Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n            \"MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n            \"JUC (Linux; U; 2.3.7; zh-cn; MB200; 320*480) UCWEB7.9.3.103/139/999\",\n            \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:7.0a1) Gecko/20110623 Firefox/7.0a1 Fennec/7.0a1\",\n            \"Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10\",\n            \"Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13\",\n            \"Mozilla/5.0 (iPhone; U; CPU iPhone OS 3_0 like Mac OS X; en-us) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/1A542a Safari/419.3\",\n            \"Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_0 like Mac OS X; en-us) AppleWebKit/532.9 (KHTML, like Gecko) Version/4.0.5 Mobile/8A293 Safari/6531.22.7\",\n            \"Mozilla/5.0 (iPad; U; CPU OS 3_2 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Version/4.0.4 Mobile/7B334b Safari/531.21.10\",\n            \"Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+\",\n            \"Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0\",\n            \"Mozilla/5.0 (SymbianOS/9.4; Series60/5.0 NokiaN97-1/20.0.019; Profile/MIDP-2.1 Configuration/CLDC-1.1) AppleWebKit/525 (KHTML, like Gecko) BrowserNG/7.1.18124\",\n            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)\",\n            \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",\n            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\",\n            \"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10\",\n            \"Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50\",\n            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2\",\n            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36\",\n            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER) \",\n            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)\",\n            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36\",\n            \"Mozilla/5.0 (Linux; U; Android 2.2.1; zh-cn; HTC_Wildfire_A3333 Build/FRG83D) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n            \"Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+\",\n            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)\",\n            \"Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999\",\n            \"Openwave/ UCWEB7.0.2.37/28/999\",\n            \"NOKIA5700/ UCWEB7.0.2.37/28/999\",\n            \"UCWEB7.0.2.37/28/999\",\n            \"Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0\",\n            \"Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13\",\n            \"Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10\",\n            \"Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\n        ]\n\n    def _generateIndexes(self):\n        numbers = random.randint(0, len(self.uas))\n        indexes = []\n        while len(indexes) < numbers:\n            temp = random.randrange(0, len(self.uas))\n            if temp not in indexes:\n                indexes.append(temp)\n        return indexes\n\n    def popupUAs(self):\n        uas = []\n        indexes = self._generateIndexes()\n        for index in indexes:\n            uas.append(self.uas[index])\n        return uas\n\n\nclass Brush(object):\n    \"\"\"\n    开刷浏览量\n    \"\"\"\n\n    def __init__(self, bloger=\"marksinoberg\"):\n        self.bloger = \"http://blog.csdn.net/{}\".format(bloger)\n        self.headers = {\n            'Host': 'blog.csdn.net',\n            'Upgrade - Insecure - Requests': '1',\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36',\n        }\n        self.rh = RedisHelper()\n\n    def getRandProxyIp(self):\n        ip = self.rh.popupAvalibeIp()\n        proxyip = {}\n        ipinfo = \"http://{}\".format(str(ip.decode('utf8')))\n        proxyip['http'] = ipinfo\n        # print(proxyip)\n        return proxyip\n\n\n    def brushLink(self, articleid, randuas=[]):\n        # http://blog.csdn.net/marksinoberg/article/details/78058279\n        bloglink = \"{}/article/details/{}\".format(self.bloger, articleid)\n        for ua in randuas:\n            self.headers['User-Agent'] = ua\n            timeseed = random.randint(1, 3)\n            print(\"临时休眠: {}秒\".format(timeseed))\n            time.sleep(timeseed)\n            for index in range(timeseed):\n                # requests.get(url=bloglink, headers=self.headers, proxies=self.getRandProxyIp())\n                requests.get(url=bloglink, headers=self.headers)\n\n\nif __name__ == '__main__':\n    # fua = FakeUserAgent()\n\n    # indexes = [0, 2,5,7]\n    # indexes = generate_random_numbers(0, 18, 7)\n    # randuas = fua.popupUAs(indexes)\n    # randuas = fua.popupUAs()\n    # print(len(randuas))\n    # print(randuas)\n\n    # print(fua._generateIndexes())\n\n    brush = Brush(\"marksinoberg\")\n    # brush.brushLink(78058279, randuas)\n    print(brush.getRandProxyIp())\n\n\n\n\n入口\n\n\n\n# coding: utf8\n\n# @Author: 郭 璞\n# @File: Main.py                                                                 \n# @Time: 2017/10/5                                   \n# @Contact: 1064319632@qq.com\n# @blog: http://blog.csdn.net/marksinoberg\n# @Description: 入口\n\nfrom csdn import *\nfrom redishelper import RedisHelper\nfrom brushhelper import *\nimport threading\n\ndef main():\n    rh = RedisHelper()\n    bs = BlogScanner(bloger=\"marksinoberg\")\n    fua = FakeUserAgent()\n    brush = Brush(bloger=\"marksinoberg\")\n\n    counter = 0\n    while counter < 12:\n        # 开刷\n        print(\"第{}次！\".format(counter))\n        try:\n            uas = fua.popupUAs()\n            articleid = rh.popupArticleId()\n            brush.brushLink(articleid, uas)\n        except Exception as e:\n            print(e)\n            # 待添加日志处理程序\n        counter+=1\n\n\nif __name__ == '__main__':\n    for i in range(280):\n        temp = threading.Thread(target=main)\n        temp.start()\n\n\n\n运行结果\n\n我拿了之前写过的一篇文章做了下测试。 \n博文链接：http://blog.csdn.net/marksinoberg/article/details/78058279\n\n开刷之前为301个浏览量，简单刷了下之后，访问量为下图：\n\n\n\n\n\n\n\n总结\n\n大致就是这个样子啦，虽然这顶多算个原型，因为代码完成度45%左右。有兴趣的可以加我QQ1064319632， 或者在评论中留下您的建议，大家一起交流，一起学习。", "time": "2018_08_13_17_28_56", "link": "https://blog.csdn.net/marksinoberg/article/details/78168223", "title": "谈谈反爬虫“政策与对策”"}
{"timestamp": "2018_08_13_17_28_57", "desc": "网飞传奇：第二曲线 \n\n\n联想浮沉：非连续性 \n\n\n刷新微软：非连续性 \n\n\n百度窘境：认知边界 \n\n\n贝索斯哲学：第一性原理 \n\n\n成为乔布斯：分形算法 \n\n\n欧洲复兴：哲科思维 \n\n\n混沌初开：灯火薪传", "time": "2018_08_13_17_28_57", "link": "https://blog.csdn.net/qq_39422642/article/details/80750854", "title": "混沌大学2018年度大课（思维导图）"}
